{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Image \n",
    "import cv2\n",
    "from sklearn.preprocessing import normalize\n",
    "from albumentations import Compose, ShiftScaleRotate, RandomGamma\n",
    "\n",
    "# Machine Learning\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE      = (256, 256, 1) # target image shape\n",
    "BATCH_SIZE = 24   # training batch size\n",
    "N_SPLITS   = 3    # k-fold splits\n",
    "EPOCHS     = 100  # training epochs\n",
    "SEED       = 1881 # Used for reproduction\n",
    "\n",
    "DIR             = 'data/OCT2017/' # DATA Location Directory\n",
    "MODEL_SAVE_PATH = \"models/\"       # K-Fold Model Path\n",
    "LOGS_PATH       = \"logs/\"         # Tensorboard Records Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCT_TrainingDataGenerator:\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape --> TUPLE.wanted image size\n",
    "    batch_size  --> INT.yielding data size for every iteration\n",
    "    orders      --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n",
    "    DIR         --> STR.DIR directory must consists \"test\", \"train\" dirs.\n",
    "    augment     --> BOOL. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, batch_size, orders, DIR, augment=True):\n",
    "        self.SHAPE       = input_shape\n",
    "        self.BATCH_SIZE  = batch_size\n",
    "        self.arr         = orders\n",
    "        self.aug         = augment\n",
    "        \n",
    "        self.DIR         = DIR\n",
    "        self.SUBDIRS     = ['train']\n",
    "        self.SUB_SUBDIRS = ['CNV', \"DME\", \"DRUSEN\", \"NORMAL\"] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.get_img_paths())\n",
    "    \n",
    "    def get_img_paths(self):\n",
    "        # Get all training image paths.\n",
    "        arr = []\n",
    "        for x in self.SUBDIRS:\n",
    "            for y in self.SUB_SUBDIRS:\n",
    "                for im in os.listdir(os.path.join(self.DIR,x,y)):\n",
    "                    if \".jpeg\" in im:\n",
    "                        arr.append(os.path.join(self.DIR,x,y,im))\n",
    "        return arr\n",
    "\n",
    "    def get_img(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        return np.array(img)\n",
    "    \n",
    "    def augmenting(self, img):\n",
    "        if self.aug:\n",
    "            augment = Compose([ShiftScaleRotate(p=0.5, shift_limit=0.01, scale_limit=0., rotate_limit=10),\n",
    "                               RandomGamma(p=0.4)])  \n",
    "        else:\n",
    "            augment = Compose([])  \n",
    "\n",
    "        img = augment(image=img)['image']\n",
    "        return img\n",
    "    \n",
    "    def resize_and_normalize(self, img):\n",
    "        img = cv2.resize(img, SHAPE[:2])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # OpenCV loads image as BGR, now we should convert it to grayscale.\n",
    "        img = normalize(img) # normalize the image in 0-1 range for faster training.\n",
    "        return np.expand_dims(img, axis=2)\n",
    "    \n",
    "    def data_generator(self):\n",
    "        img_paths = np.array(self.get_img_paths())\n",
    "        np.random.shuffle(img_paths)\n",
    "        \n",
    "        while True:\n",
    "            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float16)\n",
    "            y = np.empty((self.BATCH_SIZE, 4), dtype=np.float16)\n",
    "            \n",
    "            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n",
    "\n",
    "            for ix, id_ in enumerate(batch):\n",
    "                # x\n",
    "                img_path = img_paths[id_]\n",
    "                img = self.get_img(img_path)\n",
    "                augmented_img = self.augmenting(img)\n",
    "                img = self.resize_and_normalize(augmented_img)\n",
    "                  \n",
    "                # y \n",
    "                if 'CNV' in img_path:\n",
    "                    label = [1,0,0,0]\n",
    "                elif 'DME' in img_path:\n",
    "                    label = [0,1,0,0]\n",
    "                elif 'DRUSEN' in img_path:\n",
    "                    label = [0,0,1,0]\n",
    "                elif 'NORMAL' in img_path:\n",
    "                    label = [0,0,0,1]\n",
    "                    \n",
    "                # Store the values    \n",
    "                x[ix] = img\n",
    "                y[ix] = label\n",
    "\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.03937]\n",
      "   [0.0535 ]\n",
      "   [0.03027]\n",
      "   ...\n",
      "   [0.0444 ]\n",
      "   [0.05048]\n",
      "   [0.03937]]\n",
      "\n",
      "  [[0.04205]\n",
      "   [0.0497 ]\n",
      "   [0.0392 ]\n",
      "   ...\n",
      "   [0.04398]\n",
      "   [0.0373 ]\n",
      "   [0.0411 ]]\n",
      "\n",
      "  [[0.0358 ]\n",
      "   [0.0358 ]\n",
      "   [0.02132]\n",
      "   ...\n",
      "   [0.04092]\n",
      "   [0.03494]\n",
      "   [0.03069]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.01924]\n",
      "   [0.03848]\n",
      "   [0.03574]\n",
      "   ...\n",
      "   [0.04947]\n",
      "   [0.0577 ]\n",
      "   [0.066  ]]\n",
      "\n",
      "  [[0.03815]\n",
      "   [0.04086]\n",
      "   [0.05176]\n",
      "   ...\n",
      "   [0.04086]\n",
      "   [0.04904]\n",
      "   [0.0627 ]]\n",
      "\n",
      "  [[0.03806]\n",
      "   [0.03806]\n",
      "   [0.04892]\n",
      "   ...\n",
      "   [0.06525]\n",
      "   [0.0625 ]\n",
      "   [0.06525]]]]\n",
      "(1, 256, 256, 1)\n",
      "[[0. 0. 0. 1.]]\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Testing data generator library\n",
    "dataset = OCT_TrainingDataGenerator(SHAPE, 1, len(range(10)), DIR=DIR, augment=True)\n",
    "\n",
    "for ix, data in enumerate(dataset.data_generator()):\n",
    "    x, y = data\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    \n",
    "    if ix==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://stackoverflow.com/a/45305384\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisionx = precision(y_true, y_pred)\n",
    "    recallx = recall(y_true, y_pred)\n",
    "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452\n",
    "\n",
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://github.com/titu1994/keras-squeeze-excite-network/blob/master/se.py\n",
    "\n",
    "def squeeze_excite_block(input, ratio=16):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init._keras_shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    dropRate = 0.3\n",
    "    \n",
    "    init = Input(SHAPE)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "    x = Conv2D(64, (5, 5), activation=None, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "    x = Conv2D(128, (2, 2), activation=None, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    concat = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(concat)\n",
    "    \n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(init, x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=1e-3, momentum=0.9), metrics=['acc', precision, recall, f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\envs\\dl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1, 4)      256         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 1, 64)     256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 128, 128, 64) 0           activation_3[0][0]               \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 102464      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 64)           0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 1, 4)      256         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1, 64)     256         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 128, 128, 64) 0           activation_4[0][0]               \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 36928       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 128)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1, 1, 8)      1024        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 1, 128)    1024        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 64, 64, 128)  0           activation_6[0][0]               \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  65664       multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 128)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 1, 8)      1024        reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1, 1, 128)    1024        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 64, 64, 128)  0           activation_7[0][0]               \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 128)  147584      multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 64) 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 128 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 224 0           up_sampling2d_1[0][0]            \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 224)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          57600       global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 256)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256)          1024        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 4)            1028        activation_10[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 588,708\n",
      "Trainable params: 586,404\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** FOLD 0 **\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 2272s - loss: 0.8589 - acc: 0.6729 - precision: 0.7497 - recall: 0.5764 - f1: 0.6474 - val_loss: 0.8763 - val_acc: 0.6560 - val_precision: 0.7124 - val_recall: 0.6071 - val_f1: 0.6544\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87630, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 2/100\n",
      " - 1749s - loss: 0.5728 - acc: 0.7893 - precision: 0.8299 - recall: 0.7428 - f1: 0.7829 - val_loss: 3.1529 - val_acc: 0.5621 - val_precision: 0.5623 - val_recall: 0.5618 - val_f1: 0.5620\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.87630\n",
      "Epoch 3/100\n",
      " - 1736s - loss: 0.4445 - acc: 0.8380 - precision: 0.8612 - recall: 0.8118 - f1: 0.8352 - val_loss: 0.9460 - val_acc: 0.7998 - val_precision: 0.8093 - val_recall: 0.7952 - val_f1: 0.8020\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.87630\n",
      "Epoch 4/100\n",
      " - 1379s - loss: 0.3825 - acc: 0.8610 - precision: 0.8780 - recall: 0.8430 - f1: 0.8598 - val_loss: 0.3696 - val_acc: 0.8674 - val_precision: 0.8788 - val_recall: 0.8574 - val_f1: 0.8678\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.87630 to 0.36957, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 5/100\n",
      " - 1611s - loss: 0.3421 - acc: 0.8767 - precision: 0.8901 - recall: 0.8626 - f1: 0.8758 - val_loss: 0.6749 - val_acc: 0.7334 - val_precision: 0.7603 - val_recall: 0.6967 - val_f1: 0.7264\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36957\n",
      "Epoch 6/100\n",
      " - 1343s - loss: 0.3000 - acc: 0.8933 - precision: 0.9040 - recall: 0.8827 - f1: 0.8930 - val_loss: 0.8000 - val_acc: 0.6978 - val_precision: 0.7232 - val_recall: 0.6717 - val_f1: 0.6960\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36957\n",
      "Epoch 7/100\n",
      " - 1498s - loss: 0.2658 - acc: 0.9054 - precision: 0.9136 - recall: 0.8984 - f1: 0.9058 - val_loss: 1.0098 - val_acc: 0.6286 - val_precision: 0.6332 - val_recall: 0.6216 - val_f1: 0.6272\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.36957\n",
      "Epoch 8/100\n",
      " - 1348s - loss: 0.2495 - acc: 0.9108 - precision: 0.9178 - recall: 0.9043 - f1: 0.9109 - val_loss: 0.6936 - val_acc: 0.7714 - val_precision: 0.7961 - val_recall: 0.7492 - val_f1: 0.7714\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36957\n",
      "Epoch 9/100\n",
      " - 1299s - loss: 0.2301 - acc: 0.9188 - precision: 0.9247 - recall: 0.9131 - f1: 0.9188 - val_loss: 0.2248 - val_acc: 0.9187 - val_precision: 0.9256 - val_recall: 0.9115 - val_f1: 0.9184\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.36957 to 0.22477, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 10/100\n",
      " - 1411s - loss: 0.2198 - acc: 0.9222 - precision: 0.9277 - recall: 0.9166 - f1: 0.9220 - val_loss: 0.6658 - val_acc: 0.7257 - val_precision: 0.7619 - val_recall: 0.6959 - val_f1: 0.7266\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22477\n",
      "Epoch 11/100\n",
      " - 1391s - loss: 0.1951 - acc: 0.9316 - precision: 0.9357 - recall: 0.9274 - f1: 0.9315 - val_loss: 1.0193 - val_acc: 0.6741 - val_precision: 0.6970 - val_recall: 0.6435 - val_f1: 0.6686\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22477\n",
      "Epoch 12/100\n",
      " - 1350s - loss: 0.1859 - acc: 0.9333 - precision: 0.9377 - recall: 0.9293 - f1: 0.9334 - val_loss: 0.7293 - val_acc: 0.7759 - val_precision: 0.7861 - val_recall: 0.7633 - val_f1: 0.7743\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22477\n",
      "Epoch 13/100\n",
      " - 1356s - loss: 0.1868 - acc: 0.9347 - precision: 0.9386 - recall: 0.9311 - f1: 0.9348 - val_loss: 0.1856 - val_acc: 0.9351 - val_precision: 0.9386 - val_recall: 0.9316 - val_f1: 0.9350\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22477 to 0.18555, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 14/100\n",
      " - 1373s - loss: 0.1746 - acc: 0.9384 - precision: 0.9415 - recall: 0.9349 - f1: 0.9381 - val_loss: 0.2283 - val_acc: 0.9143 - val_precision: 0.9190 - val_recall: 0.9093 - val_f1: 0.9140\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18555\n",
      "Epoch 15/100\n",
      " - 1395s - loss: 0.1716 - acc: 0.9405 - precision: 0.9438 - recall: 0.9372 - f1: 0.9404 - val_loss: 0.3576 - val_acc: 0.8539 - val_precision: 0.8619 - val_recall: 0.8454 - val_f1: 0.8534\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18555\n",
      "Epoch 16/100\n",
      " - 1354s - loss: 0.1658 - acc: 0.9417 - precision: 0.9451 - recall: 0.9387 - f1: 0.9418 - val_loss: 0.3349 - val_acc: 0.8919 - val_precision: 0.8951 - val_recall: 0.8895 - val_f1: 0.8922\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18555\n",
      "Epoch 17/100\n",
      " - 1328s - loss: 0.1582 - acc: 0.9443 - precision: 0.9471 - recall: 0.9417 - f1: 0.9444 - val_loss: 0.4163 - val_acc: 0.8873 - val_precision: 0.8927 - val_recall: 0.8831 - val_f1: 0.8878\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18555\n",
      "Epoch 18/100\n",
      " - 1324s - loss: 0.1572 - acc: 0.9454 - precision: 0.9479 - recall: 0.9428 - f1: 0.9453 - val_loss: 0.1785 - val_acc: 0.9401 - val_precision: 0.9425 - val_recall: 0.9377 - val_f1: 0.9400\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.18555 to 0.17848, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 19/100\n",
      " - 1321s - loss: 0.1540 - acc: 0.9455 - precision: 0.9482 - recall: 0.9427 - f1: 0.9454 - val_loss: 0.1718 - val_acc: 0.9424 - val_precision: 0.9483 - val_recall: 0.9364 - val_f1: 0.9422\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.17848 to 0.17180, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 20/100\n",
      " - 1311s - loss: 0.1469 - acc: 0.9480 - precision: 0.9507 - recall: 0.9453 - f1: 0.9479 - val_loss: 0.1279 - val_acc: 0.9540 - val_precision: 0.9562 - val_recall: 0.9521 - val_f1: 0.9541\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.17180 to 0.12791, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 21/100\n",
      " - 1321s - loss: 0.1414 - acc: 0.9497 - precision: 0.9523 - recall: 0.9476 - f1: 0.9499 - val_loss: 0.2148 - val_acc: 0.9208 - val_precision: 0.9270 - val_recall: 0.9162 - val_f1: 0.9215\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12791\n",
      "Epoch 22/100\n",
      " - 1298s - loss: 0.1382 - acc: 0.9508 - precision: 0.9526 - recall: 0.9485 - f1: 0.9505 - val_loss: 0.4052 - val_acc: 0.8539 - val_precision: 0.8608 - val_recall: 0.8485 - val_f1: 0.8545\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12791\n",
      "Epoch 23/100\n",
      " - 1371s - loss: 0.1400 - acc: 0.9502 - precision: 0.9527 - recall: 0.9483 - f1: 0.9505 - val_loss: 0.1650 - val_acc: 0.9379 - val_precision: 0.9413 - val_recall: 0.9350 - val_f1: 0.9380\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12791\n",
      "Epoch 24/100\n",
      " - 1353s - loss: 0.1359 - acc: 0.9522 - precision: 0.9539 - recall: 0.9501 - f1: 0.9520 - val_loss: 0.1904 - val_acc: 0.9281 - val_precision: 0.9326 - val_recall: 0.9257 - val_f1: 0.9291\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12791\n",
      "Epoch 25/100\n",
      " - 1316s - loss: 0.1338 - acc: 0.9524 - precision: 0.9543 - recall: 0.9505 - f1: 0.9524 - val_loss: 0.1812 - val_acc: 0.9381 - val_precision: 0.9430 - val_recall: 0.9345 - val_f1: 0.9386\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12791\n",
      "Epoch 26/100\n",
      " - 1427s - loss: 0.1296 - acc: 0.9544 - precision: 0.9564 - recall: 0.9527 - f1: 0.9545 - val_loss: 0.2656 - val_acc: 0.9039 - val_precision: 0.9127 - val_recall: 0.8953 - val_f1: 0.9038\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12791\n",
      "Epoch 27/100\n",
      " - 1331s - loss: 0.1294 - acc: 0.9536 - precision: 0.9554 - recall: 0.9517 - f1: 0.9535 - val_loss: 0.2065 - val_acc: 0.9211 - val_precision: 0.9257 - val_recall: 0.9166 - val_f1: 0.9210\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12791\n",
      "Epoch 28/100\n",
      " - 1312s - loss: 0.1243 - acc: 0.9556 - precision: 0.9575 - recall: 0.9542 - f1: 0.9558 - val_loss: 0.1724 - val_acc: 0.9373 - val_precision: 0.9417 - val_recall: 0.9329 - val_f1: 0.9372\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12791\n",
      "Epoch 29/100\n",
      " - 1571s - loss: 0.1208 - acc: 0.9569 - precision: 0.9587 - recall: 0.9550 - f1: 0.9568 - val_loss: 0.1572 - val_acc: 0.9449 - val_precision: 0.9475 - val_recall: 0.9422 - val_f1: 0.9448\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12791\n",
      "Epoch 30/100\n",
      " - 1311s - loss: 0.1204 - acc: 0.9579 - precision: 0.9596 - recall: 0.9561 - f1: 0.9578 - val_loss: 0.1167 - val_acc: 0.9576 - val_precision: 0.9589 - val_recall: 0.9567 - val_f1: 0.9578\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12791 to 0.11669, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 31/100\n",
      " - 1322s - loss: 0.1169 - acc: 0.9580 - precision: 0.9594 - recall: 0.9565 - f1: 0.9579 - val_loss: 0.1390 - val_acc: 0.9488 - val_precision: 0.9509 - val_recall: 0.9469 - val_f1: 0.9488\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11669\n",
      "Epoch 32/100\n",
      " - 1298s - loss: 0.1136 - acc: 0.9596 - precision: 0.9611 - recall: 0.9584 - f1: 0.9597 - val_loss: 0.2024 - val_acc: 0.9260 - val_precision: 0.9315 - val_recall: 0.9235 - val_f1: 0.9274\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11669\n",
      "Epoch 33/100\n",
      " - 2211s - loss: 0.1141 - acc: 0.9598 - precision: 0.9612 - recall: 0.9585 - f1: 0.9598 - val_loss: 0.1253 - val_acc: 0.9565 - val_precision: 0.9579 - val_recall: 0.9548 - val_f1: 0.9563\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11669\n",
      "Epoch 34/100\n",
      " - 2288s - loss: 0.1111 - acc: 0.9609 - precision: 0.9622 - recall: 0.9594 - f1: 0.9608 - val_loss: 0.1212 - val_acc: 0.9584 - val_precision: 0.9601 - val_recall: 0.9565 - val_f1: 0.9582\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11669\n",
      "Epoch 35/100\n",
      " - 1909s - loss: 0.1071 - acc: 0.9617 - precision: 0.9629 - recall: 0.9603 - f1: 0.9616 - val_loss: 0.1073 - val_acc: 0.9625 - val_precision: 0.9637 - val_recall: 0.9618 - val_f1: 0.9628\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11669 to 0.10732, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 36/100\n",
      " - 1524s - loss: 0.1086 - acc: 0.9617 - precision: 0.9629 - recall: 0.9605 - f1: 0.9617 - val_loss: 0.1826 - val_acc: 0.9321 - val_precision: 0.9351 - val_recall: 0.9293 - val_f1: 0.9321\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10732\n",
      "Epoch 37/100\n",
      " - 1480s - loss: 0.1060 - acc: 0.9630 - precision: 0.9645 - recall: 0.9616 - f1: 0.9631 - val_loss: 0.1759 - val_acc: 0.9382 - val_precision: 0.9412 - val_recall: 0.9353 - val_f1: 0.9382\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10732\n",
      "Epoch 38/100\n",
      " - 1432s - loss: 0.1021 - acc: 0.9634 - precision: 0.9649 - recall: 0.9625 - f1: 0.9636 - val_loss: 0.1334 - val_acc: 0.9512 - val_precision: 0.9522 - val_recall: 0.9500 - val_f1: 0.9510\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.10732\n",
      "Epoch 39/100\n",
      " - 1429s - loss: 0.0993 - acc: 0.9659 - precision: 0.9669 - recall: 0.9650 - f1: 0.9659 - val_loss: 0.0958 - val_acc: 0.9666 - val_precision: 0.9678 - val_recall: 0.9652 - val_f1: 0.9665\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10732 to 0.09577, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 40/100\n",
      " - 1395s - loss: 0.1017 - acc: 0.9631 - precision: 0.9646 - recall: 0.9618 - f1: 0.9632 - val_loss: 0.1442 - val_acc: 0.9501 - val_precision: 0.9521 - val_recall: 0.9475 - val_f1: 0.9498\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09577\n",
      "Epoch 41/100\n",
      " - 1336s - loss: 0.0966 - acc: 0.9659 - precision: 0.9670 - recall: 0.9649 - f1: 0.9659 - val_loss: 0.0939 - val_acc: 0.9677 - val_precision: 0.9686 - val_recall: 0.9669 - val_f1: 0.9677\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09577 to 0.09390, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 42/100\n",
      " - 1362s - loss: 0.0968 - acc: 0.9656 - precision: 0.9670 - recall: 0.9643 - f1: 0.9656 - val_loss: 0.1264 - val_acc: 0.9542 - val_precision: 0.9569 - val_recall: 0.9518 - val_f1: 0.9543\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09390\n",
      "Epoch 43/100\n",
      " - 1413s - loss: 0.0947 - acc: 0.9660 - precision: 0.9672 - recall: 0.9648 - f1: 0.9660 - val_loss: 0.0994 - val_acc: 0.9658 - val_precision: 0.9670 - val_recall: 0.9642 - val_f1: 0.9656\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09390\n",
      "Epoch 44/100\n",
      " - 1300s - loss: 0.0924 - acc: 0.9674 - precision: 0.9685 - recall: 0.9663 - f1: 0.9674 - val_loss: 0.0876 - val_acc: 0.9690 - val_precision: 0.9705 - val_recall: 0.9685 - val_f1: 0.9695\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09390 to 0.08755, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 45/100\n",
      " - 1356s - loss: 0.0914 - acc: 0.9678 - precision: 0.9687 - recall: 0.9670 - f1: 0.9678 - val_loss: 0.1415 - val_acc: 0.9495 - val_precision: 0.9527 - val_recall: 0.9473 - val_f1: 0.9499\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08755\n",
      "Epoch 46/100\n",
      " - 1305s - loss: 0.0903 - acc: 0.9685 - precision: 0.9696 - recall: 0.9676 - f1: 0.9685 - val_loss: 0.1300 - val_acc: 0.9592 - val_precision: 0.9599 - val_recall: 0.9587 - val_f1: 0.9593\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08755\n",
      "Epoch 47/100\n",
      " - 1336s - loss: 0.0918 - acc: 0.9678 - precision: 0.9687 - recall: 0.9668 - f1: 0.9677 - val_loss: 0.2668 - val_acc: 0.9099 - val_precision: 0.9128 - val_recall: 0.9079 - val_f1: 0.9103\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08755\n",
      "Epoch 48/100\n",
      " - 1413s - loss: 0.0880 - acc: 0.9681 - precision: 0.9689 - recall: 0.9669 - f1: 0.9679 - val_loss: 0.2022 - val_acc: 0.9288 - val_precision: 0.9312 - val_recall: 0.9263 - val_f1: 0.9287\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.08755\n",
      "Epoch 49/100\n",
      " - 1321s - loss: 0.0904 - acc: 0.9683 - precision: 0.9693 - recall: 0.9673 - f1: 0.9683 - val_loss: 0.0787 - val_acc: 0.9726 - val_precision: 0.9735 - val_recall: 0.9715 - val_f1: 0.9725\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.08755 to 0.07867, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 50/100\n",
      " - 1402s - loss: 0.0847 - acc: 0.9685 - precision: 0.9695 - recall: 0.9679 - f1: 0.9687 - val_loss: 0.1015 - val_acc: 0.9628 - val_precision: 0.9639 - val_recall: 0.9615 - val_f1: 0.9627\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07867\n",
      "Epoch 51/100\n",
      " - 1329s - loss: 0.0815 - acc: 0.9713 - precision: 0.9722 - recall: 0.9704 - f1: 0.9713 - val_loss: 0.1009 - val_acc: 0.9627 - val_precision: 0.9635 - val_recall: 0.9615 - val_f1: 0.9625\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07867\n",
      "Epoch 52/100\n",
      " - 1333s - loss: 0.0834 - acc: 0.9704 - precision: 0.9712 - recall: 0.9695 - f1: 0.9703 - val_loss: 0.0848 - val_acc: 0.9698 - val_precision: 0.9708 - val_recall: 0.9691 - val_f1: 0.9699\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07867\n",
      "Epoch 53/100\n",
      " - 1499s - loss: 0.0863 - acc: 0.9695 - precision: 0.9703 - recall: 0.9684 - f1: 0.9693 - val_loss: 0.1603 - val_acc: 0.9451 - val_precision: 0.9461 - val_recall: 0.9440 - val_f1: 0.9450\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07867\n",
      "Epoch 54/100\n",
      " - 1409s - loss: 0.0810 - acc: 0.9707 - precision: 0.9716 - recall: 0.9701 - f1: 0.9708 - val_loss: 0.0990 - val_acc: 0.9674 - val_precision: 0.9687 - val_recall: 0.9670 - val_f1: 0.9678\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07867\n",
      "Epoch 55/100\n",
      " - 1304s - loss: 0.0818 - acc: 0.9703 - precision: 0.9714 - recall: 0.9696 - f1: 0.9704 - val_loss: 0.1255 - val_acc: 0.9604 - val_precision: 0.9615 - val_recall: 0.9601 - val_f1: 0.9608\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07867\n",
      "Epoch 56/100\n",
      " - 1324s - loss: 0.0823 - acc: 0.9700 - precision: 0.9711 - recall: 0.9690 - f1: 0.9700 - val_loss: 0.1069 - val_acc: 0.9632 - val_precision: 0.9641 - val_recall: 0.9628 - val_f1: 0.9634\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07867\n",
      "Epoch 57/100\n",
      " - 1319s - loss: 0.0831 - acc: 0.9701 - precision: 0.9709 - recall: 0.9694 - f1: 0.9701 - val_loss: 0.0979 - val_acc: 0.9657 - val_precision: 0.9665 - val_recall: 0.9653 - val_f1: 0.9658\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07867\n",
      "Epoch 58/100\n",
      " - 1409s - loss: 0.0797 - acc: 0.9710 - precision: 0.9718 - recall: 0.9700 - f1: 0.9709 - val_loss: 0.1100 - val_acc: 0.9632 - val_precision: 0.9638 - val_recall: 0.9622 - val_f1: 0.9630\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07867\n",
      "Epoch 59/100\n",
      " - 1698s - loss: 0.0790 - acc: 0.9723 - precision: 0.9731 - recall: 0.9714 - f1: 0.9722 - val_loss: 0.4182 - val_acc: 0.8450 - val_precision: 0.8477 - val_recall: 0.8425 - val_f1: 0.8450\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07867\n",
      "Epoch 60/100\n",
      " - 1725s - loss: 0.0788 - acc: 0.9718 - precision: 0.9728 - recall: 0.9710 - f1: 0.9719 - val_loss: 0.0831 - val_acc: 0.9699 - val_precision: 0.9705 - val_recall: 0.9695 - val_f1: 0.9700\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07867\n",
      "Epoch 61/100\n",
      " - 1652s - loss: 0.0775 - acc: 0.9727 - precision: 0.9734 - recall: 0.9721 - f1: 0.9727 - val_loss: 0.2121 - val_acc: 0.9323 - val_precision: 0.9341 - val_recall: 0.9310 - val_f1: 0.9325\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07867\n",
      "Epoch 62/100\n",
      " - 1354s - loss: 0.0761 - acc: 0.9723 - precision: 0.9731 - recall: 0.9714 - f1: 0.9722 - val_loss: 0.0756 - val_acc: 0.9717 - val_precision: 0.9722 - val_recall: 0.9716 - val_f1: 0.9719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00062: val_loss improved from 0.07867 to 0.07555, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 63/100\n",
      " - 1603s - loss: 0.0715 - acc: 0.9749 - precision: 0.9758 - recall: 0.9741 - f1: 0.9750 - val_loss: 0.0695 - val_acc: 0.9761 - val_precision: 0.9769 - val_recall: 0.9754 - val_f1: 0.9761\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.07555 to 0.06947, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 64/100\n",
      " - 1509s - loss: 0.0722 - acc: 0.9742 - precision: 0.9749 - recall: 0.9734 - f1: 0.9742 - val_loss: 0.1157 - val_acc: 0.9639 - val_precision: 0.9646 - val_recall: 0.9632 - val_f1: 0.9639\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.06947\n",
      "Epoch 65/100\n",
      " - 1448s - loss: 0.0758 - acc: 0.9731 - precision: 0.9738 - recall: 0.9725 - f1: 0.9731 - val_loss: 0.0803 - val_acc: 0.9729 - val_precision: 0.9733 - val_recall: 0.9725 - val_f1: 0.9729\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.06947\n",
      "Epoch 66/100\n",
      " - 1337s - loss: 0.0730 - acc: 0.9745 - precision: 0.9751 - recall: 0.9738 - f1: 0.9745 - val_loss: 0.1518 - val_acc: 0.9446 - val_precision: 0.9467 - val_recall: 0.9415 - val_f1: 0.9440\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.06947\n",
      "Epoch 67/100\n",
      " - 1367s - loss: 0.0740 - acc: 0.9737 - precision: 0.9746 - recall: 0.9732 - f1: 0.9739 - val_loss: 0.0722 - val_acc: 0.9750 - val_precision: 0.9756 - val_recall: 0.9744 - val_f1: 0.9750\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.06947\n",
      "Epoch 68/100\n",
      " - 1392s - loss: 0.0747 - acc: 0.9725 - precision: 0.9733 - recall: 0.9719 - f1: 0.9726 - val_loss: 0.1351 - val_acc: 0.9504 - val_precision: 0.9523 - val_recall: 0.9488 - val_f1: 0.9505\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.06947\n",
      "Epoch 69/100\n",
      " - 1369s - loss: 0.0742 - acc: 0.9736 - precision: 0.9743 - recall: 0.9727 - f1: 0.9735 - val_loss: 0.0926 - val_acc: 0.9666 - val_precision: 0.9673 - val_recall: 0.9653 - val_f1: 0.9662\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.06947\n",
      "Epoch 70/100\n",
      " - 1574s - loss: 0.0702 - acc: 0.9757 - precision: 0.9762 - recall: 0.9751 - f1: 0.9756 - val_loss: 0.0869 - val_acc: 0.9672 - val_precision: 0.9682 - val_recall: 0.9669 - val_f1: 0.9676\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.06947\n",
      "Epoch 71/100\n",
      " - 1318s - loss: 0.0696 - acc: 0.9753 - precision: 0.9761 - recall: 0.9748 - f1: 0.9754 - val_loss: 0.0783 - val_acc: 0.9710 - val_precision: 0.9719 - val_recall: 0.9703 - val_f1: 0.9711\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.06947\n",
      "Epoch 72/100\n",
      " - 1529s - loss: 0.0670 - acc: 0.9764 - precision: 0.9773 - recall: 0.9757 - f1: 0.9765 - val_loss: 0.1018 - val_acc: 0.9671 - val_precision: 0.9683 - val_recall: 0.9661 - val_f1: 0.9672\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.06947\n",
      "Epoch 73/100\n",
      " - 1329s - loss: 0.0683 - acc: 0.9762 - precision: 0.9770 - recall: 0.9753 - f1: 0.9761 - val_loss: 0.0921 - val_acc: 0.9706 - val_precision: 0.9720 - val_recall: 0.9701 - val_f1: 0.9710\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.06947\n",
      "Epoch 74/100\n",
      " - 1298s - loss: 0.0663 - acc: 0.9761 - precision: 0.9767 - recall: 0.9753 - f1: 0.9760 - val_loss: 0.0829 - val_acc: 0.9714 - val_precision: 0.9721 - val_recall: 0.9703 - val_f1: 0.9712\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.06947\n",
      "Epoch 75/100\n",
      " - 1295s - loss: 0.0627 - acc: 0.9785 - precision: 0.9791 - recall: 0.9778 - f1: 0.9784 - val_loss: 0.0624 - val_acc: 0.9796 - val_precision: 0.9800 - val_recall: 0.9790 - val_f1: 0.9795\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.06947 to 0.06240, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 76/100\n",
      " - 1326s - loss: 0.0650 - acc: 0.9768 - precision: 0.9775 - recall: 0.9763 - f1: 0.9769 - val_loss: 0.2055 - val_acc: 0.9251 - val_precision: 0.9270 - val_recall: 0.9229 - val_f1: 0.9249\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.06240\n",
      "Epoch 77/100\n",
      " - 1353s - loss: 0.0655 - acc: 0.9772 - precision: 0.9777 - recall: 0.9765 - f1: 0.9771 - val_loss: 0.0825 - val_acc: 0.9730 - val_precision: 0.9737 - val_recall: 0.9725 - val_f1: 0.9730\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.06240\n",
      "Epoch 78/100\n",
      " - 1421s - loss: 0.0649 - acc: 0.9770 - precision: 0.9776 - recall: 0.9764 - f1: 0.9770 - val_loss: 0.0743 - val_acc: 0.9739 - val_precision: 0.9747 - val_recall: 0.9730 - val_f1: 0.9738\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.06240\n",
      "Epoch 79/100\n",
      " - 2655s - loss: 0.0637 - acc: 0.9773 - precision: 0.9778 - recall: 0.9767 - f1: 0.9772 - val_loss: 0.0770 - val_acc: 0.9739 - val_precision: 0.9749 - val_recall: 0.9731 - val_f1: 0.9740\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.06240\n",
      "Epoch 80/100\n",
      " - 1905s - loss: 0.0655 - acc: 0.9764 - precision: 0.9772 - recall: 0.9759 - f1: 0.9766 - val_loss: 0.0560 - val_acc: 0.9817 - val_precision: 0.9823 - val_recall: 0.9813 - val_f1: 0.9818\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.06240 to 0.05596, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 81/100\n",
      " - 1624s - loss: 0.0627 - acc: 0.9776 - precision: 0.9783 - recall: 0.9769 - f1: 0.9776 - val_loss: 0.0746 - val_acc: 0.9753 - val_precision: 0.9757 - val_recall: 0.9745 - val_f1: 0.9751\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.05596\n",
      "Epoch 82/100\n",
      " - 1370s - loss: 0.0611 - acc: 0.9779 - precision: 0.9784 - recall: 0.9773 - f1: 0.9778 - val_loss: 0.0655 - val_acc: 0.9781 - val_precision: 0.9788 - val_recall: 0.9779 - val_f1: 0.9784\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.05596\n",
      "Epoch 83/100\n",
      " - 1467s - loss: 0.0621 - acc: 0.9780 - precision: 0.9786 - recall: 0.9776 - f1: 0.9781 - val_loss: 0.0588 - val_acc: 0.9799 - val_precision: 0.9802 - val_recall: 0.9797 - val_f1: 0.9800\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.05596\n",
      "Epoch 84/100\n",
      " - 1331s - loss: 0.0598 - acc: 0.9780 - precision: 0.9787 - recall: 0.9775 - f1: 0.9781 - val_loss: 0.0645 - val_acc: 0.9788 - val_precision: 0.9789 - val_recall: 0.9785 - val_f1: 0.9787\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.05596\n",
      "Epoch 85/100\n",
      " - 1336s - loss: 0.0580 - acc: 0.9797 - precision: 0.9803 - recall: 0.9792 - f1: 0.9797 - val_loss: 0.0581 - val_acc: 0.9805 - val_precision: 0.9807 - val_recall: 0.9800 - val_f1: 0.9804\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.05596\n",
      "Epoch 86/100\n",
      " - 1302s - loss: 0.0599 - acc: 0.9786 - precision: 0.9791 - recall: 0.9780 - f1: 0.9786 - val_loss: 0.0559 - val_acc: 0.9801 - val_precision: 0.9802 - val_recall: 0.9798 - val_f1: 0.9800\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.05596 to 0.05589, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 87/100\n",
      " - 1390s - loss: 0.0566 - acc: 0.9801 - precision: 0.9808 - recall: 0.9798 - f1: 0.9803 - val_loss: 0.0861 - val_acc: 0.9689 - val_precision: 0.9694 - val_recall: 0.9683 - val_f1: 0.9689\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.05589\n",
      "Epoch 88/100\n",
      " - 1319s - loss: 0.0607 - acc: 0.9781 - precision: 0.9786 - recall: 0.9773 - f1: 0.9779 - val_loss: 0.0979 - val_acc: 0.9636 - val_precision: 0.9642 - val_recall: 0.9632 - val_f1: 0.9637\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.05589\n",
      "Epoch 89/100\n",
      " - 1400s - loss: 0.0614 - acc: 0.9783 - precision: 0.9787 - recall: 0.9778 - f1: 0.9782 - val_loss: 0.0574 - val_acc: 0.9809 - val_precision: 0.9810 - val_recall: 0.9807 - val_f1: 0.9809\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.05589\n",
      "Epoch 90/100\n",
      " - 1319s - loss: 0.0588 - acc: 0.9789 - precision: 0.9794 - recall: 0.9784 - f1: 0.9789 - val_loss: 0.0721 - val_acc: 0.9747 - val_precision: 0.9754 - val_recall: 0.9741 - val_f1: 0.9747\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.05589\n",
      "Epoch 91/100\n",
      " - 1306s - loss: 0.0565 - acc: 0.9794 - precision: 0.9800 - recall: 0.9790 - f1: 0.9795 - val_loss: 0.0500 - val_acc: 0.9840 - val_precision: 0.9843 - val_recall: 0.9839 - val_f1: 0.9841\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.05589 to 0.04996, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 92/100\n",
      " - 1396s - loss: 0.0565 - acc: 0.9792 - precision: 0.9797 - recall: 0.9787 - f1: 0.9792 - val_loss: 0.0664 - val_acc: 0.9776 - val_precision: 0.9781 - val_recall: 0.9768 - val_f1: 0.9774\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.04996\n",
      "Epoch 93/100\n",
      " - 1322s - loss: 0.0545 - acc: 0.9805 - precision: 0.9812 - recall: 0.9801 - f1: 0.9806 - val_loss: 0.1577 - val_acc: 0.9413 - val_precision: 0.9427 - val_recall: 0.9398 - val_f1: 0.9412\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.04996\n",
      "Epoch 94/100\n",
      " - 1326s - loss: 0.0555 - acc: 0.9805 - precision: 0.9810 - recall: 0.9799 - f1: 0.9804 - val_loss: 0.0828 - val_acc: 0.9713 - val_precision: 0.9719 - val_recall: 0.9711 - val_f1: 0.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00094: val_loss did not improve from 0.04996\n",
      "Epoch 95/100\n",
      " - 1589s - loss: 0.0556 - acc: 0.9804 - precision: 0.9809 - recall: 0.9799 - f1: 0.9804 - val_loss: 0.0487 - val_acc: 0.9844 - val_precision: 0.9846 - val_recall: 0.9843 - val_f1: 0.9844\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.04996 to 0.04866, saving model to models/heysaw_fold_0.h5\n",
      "Epoch 96/100\n",
      " - 1413s - loss: 0.0553 - acc: 0.9808 - precision: 0.9813 - recall: 0.9803 - f1: 0.9808 - val_loss: 0.1336 - val_acc: 0.9574 - val_precision: 0.9582 - val_recall: 0.9561 - val_f1: 0.9571\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.04866\n",
      "Epoch 97/100\n",
      " - 1319s - loss: 0.0542 - acc: 0.9805 - precision: 0.9810 - recall: 0.9800 - f1: 0.9805 - val_loss: 0.0722 - val_acc: 0.9754 - val_precision: 0.9757 - val_recall: 0.9752 - val_f1: 0.9754\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.04866\n",
      "Epoch 98/100\n",
      " - 1593s - loss: 0.0555 - acc: 0.9806 - precision: 0.9810 - recall: 0.9801 - f1: 0.9805 - val_loss: 0.0656 - val_acc: 0.9785 - val_precision: 0.9794 - val_recall: 0.9781 - val_f1: 0.9787\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.04866\n",
      "Epoch 99/100\n",
      " - 1351s - loss: 0.0543 - acc: 0.9811 - precision: 0.9815 - recall: 0.9806 - f1: 0.9810 - val_loss: 0.0673 - val_acc: 0.9768 - val_precision: 0.9771 - val_recall: 0.9765 - val_f1: 0.9768\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.04866\n",
      "Epoch 100/100\n",
      " - 1455s - loss: 0.0544 - acc: 0.9803 - precision: 0.9808 - recall: 0.9799 - f1: 0.9803 - val_loss: 0.0572 - val_acc: 0.9811 - val_precision: 0.9813 - val_recall: 0.9808 - val_f1: 0.9810\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04866\n",
      "** FOLD 1 **\n",
      "Epoch 1/100\n",
      " - 1790s - loss: 0.1223 - acc: 0.9570 - precision: 0.9582 - recall: 0.9560 - f1: 0.9571 - val_loss: 3.3613 - val_acc: 0.3562 - val_precision: 0.3552 - val_recall: 0.3405 - val_f1: 0.3475\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.36129, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 2/100\n",
      " - 1657s - loss: 0.1147 - acc: 0.9601 - precision: 0.9612 - recall: 0.9587 - f1: 0.9599 - val_loss: 0.1559 - val_acc: 0.9500 - val_precision: 0.9522 - val_recall: 0.9486 - val_f1: 0.9503\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.36129 to 0.15591, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 3/100\n",
      " - 1413s - loss: 0.1066 - acc: 0.9625 - precision: 0.9638 - recall: 0.9615 - f1: 0.9626 - val_loss: 0.2009 - val_acc: 0.9240 - val_precision: 0.9270 - val_recall: 0.9212 - val_f1: 0.9240\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15591\n",
      "Epoch 4/100\n",
      " - 1429s - loss: 0.1035 - acc: 0.9632 - precision: 0.9646 - recall: 0.9625 - f1: 0.9635 - val_loss: 0.1497 - val_acc: 0.9446 - val_precision: 0.9469 - val_recall: 0.9426 - val_f1: 0.9447\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15591 to 0.14966, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 5/100\n",
      " - 1380s - loss: 0.1034 - acc: 0.9638 - precision: 0.9649 - recall: 0.9627 - f1: 0.9637 - val_loss: 0.1711 - val_acc: 0.9366 - val_precision: 0.9391 - val_recall: 0.9354 - val_f1: 0.9372\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14966\n",
      "Epoch 6/100\n",
      " - 1401s - loss: 0.0990 - acc: 0.9649 - precision: 0.9659 - recall: 0.9637 - f1: 0.9648 - val_loss: 0.2145 - val_acc: 0.9202 - val_precision: 0.9230 - val_recall: 0.9175 - val_f1: 0.9202\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.14966\n",
      "Epoch 7/100\n",
      " - 1365s - loss: 0.0977 - acc: 0.9652 - precision: 0.9662 - recall: 0.9642 - f1: 0.9652 - val_loss: 0.0943 - val_acc: 0.9666 - val_precision: 0.9680 - val_recall: 0.9656 - val_f1: 0.9668\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.14966 to 0.09433, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 8/100\n",
      " - 1396s - loss: 0.0949 - acc: 0.9663 - precision: 0.9672 - recall: 0.9655 - f1: 0.9663 - val_loss: 0.1227 - val_acc: 0.9547 - val_precision: 0.9560 - val_recall: 0.9536 - val_f1: 0.9548\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09433\n",
      "Epoch 9/100\n",
      " - 1375s - loss: 0.0923 - acc: 0.9665 - precision: 0.9676 - recall: 0.9657 - f1: 0.9667 - val_loss: 0.0681 - val_acc: 0.9768 - val_precision: 0.9777 - val_recall: 0.9764 - val_f1: 0.9771\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09433 to 0.06807, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 10/100\n",
      " - 1383s - loss: 0.0889 - acc: 0.9679 - precision: 0.9689 - recall: 0.9672 - f1: 0.9680 - val_loss: 0.2443 - val_acc: 0.9092 - val_precision: 0.9117 - val_recall: 0.9077 - val_f1: 0.9097\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06807\n",
      "Epoch 11/100\n",
      " - 1438s - loss: 0.0859 - acc: 0.9697 - precision: 0.9706 - recall: 0.9690 - f1: 0.9698 - val_loss: 0.0705 - val_acc: 0.9758 - val_precision: 0.9763 - val_recall: 0.9749 - val_f1: 0.9756\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.06807\n",
      "Epoch 12/100\n",
      " - 1427s - loss: 0.0853 - acc: 0.9703 - precision: 0.9713 - recall: 0.9696 - f1: 0.9704 - val_loss: 0.1090 - val_acc: 0.9594 - val_precision: 0.9598 - val_recall: 0.9587 - val_f1: 0.9592\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.06807\n",
      "Epoch 13/100\n",
      " - 1398s - loss: 0.0840 - acc: 0.9704 - precision: 0.9715 - recall: 0.9700 - f1: 0.9707 - val_loss: 0.0825 - val_acc: 0.9702 - val_precision: 0.9708 - val_recall: 0.9698 - val_f1: 0.9703\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06807\n",
      "Epoch 14/100\n",
      " - 1398s - loss: 0.0809 - acc: 0.9716 - precision: 0.9723 - recall: 0.9707 - f1: 0.9715 - val_loss: 0.0899 - val_acc: 0.9690 - val_precision: 0.9705 - val_recall: 0.9676 - val_f1: 0.9690\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06807\n",
      "Epoch 15/100\n",
      " - 1433s - loss: 0.0820 - acc: 0.9704 - precision: 0.9712 - recall: 0.9696 - f1: 0.9704 - val_loss: 0.0861 - val_acc: 0.9716 - val_precision: 0.9724 - val_recall: 0.9707 - val_f1: 0.9715\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06807\n",
      "Epoch 16/100\n",
      " - 1379s - loss: 0.0762 - acc: 0.9721 - precision: 0.9727 - recall: 0.9713 - f1: 0.9720 - val_loss: 0.1708 - val_acc: 0.9404 - val_precision: 0.9427 - val_recall: 0.9383 - val_f1: 0.9404\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06807\n",
      "Epoch 17/100\n",
      " - 1459s - loss: 0.0781 - acc: 0.9718 - precision: 0.9726 - recall: 0.9712 - f1: 0.9719 - val_loss: 0.0810 - val_acc: 0.9709 - val_precision: 0.9722 - val_recall: 0.9701 - val_f1: 0.9711\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06807\n",
      "Epoch 18/100\n",
      " - 1547s - loss: 0.0764 - acc: 0.9734 - precision: 0.9738 - recall: 0.9726 - f1: 0.9732 - val_loss: 0.2380 - val_acc: 0.9347 - val_precision: 0.9366 - val_recall: 0.9334 - val_f1: 0.9350\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06807\n",
      "Epoch 19/100\n",
      " - 1411s - loss: 0.0783 - acc: 0.9720 - precision: 0.9725 - recall: 0.9715 - f1: 0.9720 - val_loss: 0.1518 - val_acc: 0.9447 - val_precision: 0.9465 - val_recall: 0.9434 - val_f1: 0.9449\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06807\n",
      "Epoch 20/100\n",
      " - 1429s - loss: 0.0709 - acc: 0.9740 - precision: 0.9747 - recall: 0.9731 - f1: 0.9739 - val_loss: 0.1360 - val_acc: 0.9495 - val_precision: 0.9511 - val_recall: 0.9484 - val_f1: 0.9497\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06807\n",
      "Epoch 21/100\n",
      " - 1468s - loss: 0.0702 - acc: 0.9751 - precision: 0.9758 - recall: 0.9746 - f1: 0.9751 - val_loss: 0.1110 - val_acc: 0.9587 - val_precision: 0.9589 - val_recall: 0.9584 - val_f1: 0.9587\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06807\n",
      "Epoch 22/100\n",
      " - 1519s - loss: 0.0661 - acc: 0.9766 - precision: 0.9772 - recall: 0.9761 - f1: 0.9766 - val_loss: 0.1188 - val_acc: 0.9568 - val_precision: 0.9573 - val_recall: 0.9560 - val_f1: 0.9566\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06807\n",
      "Epoch 23/100\n",
      " - 1486s - loss: 0.0663 - acc: 0.9764 - precision: 0.9768 - recall: 0.9760 - f1: 0.9764 - val_loss: 0.0893 - val_acc: 0.9680 - val_precision: 0.9686 - val_recall: 0.9676 - val_f1: 0.9681\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06807\n",
      "Epoch 24/100\n",
      " - 1463s - loss: 0.0687 - acc: 0.9753 - precision: 0.9758 - recall: 0.9749 - f1: 0.9754 - val_loss: 0.1037 - val_acc: 0.9620 - val_precision: 0.9631 - val_recall: 0.9612 - val_f1: 0.9621\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06807\n",
      "Epoch 25/100\n",
      " - 1388s - loss: 0.0649 - acc: 0.9773 - precision: 0.9777 - recall: 0.9769 - f1: 0.9773 - val_loss: 0.0788 - val_acc: 0.9700 - val_precision: 0.9708 - val_recall: 0.9695 - val_f1: 0.9701\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06807\n",
      "Epoch 26/100\n",
      " - 1387s - loss: 0.0641 - acc: 0.9770 - precision: 0.9776 - recall: 0.9765 - f1: 0.9770 - val_loss: 0.0944 - val_acc: 0.9688 - val_precision: 0.9700 - val_recall: 0.9682 - val_f1: 0.9691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss did not improve from 0.06807\n",
      "Epoch 27/100\n",
      " - 1441s - loss: 0.0641 - acc: 0.9776 - precision: 0.9780 - recall: 0.9769 - f1: 0.9775 - val_loss: 0.0645 - val_acc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9783 - val_f1: 0.9787\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.06807 to 0.06451, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 28/100\n",
      " - 1543s - loss: 0.0629 - acc: 0.9769 - precision: 0.9774 - recall: 0.9764 - f1: 0.9769 - val_loss: 0.0921 - val_acc: 0.9681 - val_precision: 0.9691 - val_recall: 0.9670 - val_f1: 0.9680\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.06451\n",
      "Epoch 29/100\n",
      " - 1441s - loss: 0.0643 - acc: 0.9772 - precision: 0.9778 - recall: 0.9765 - f1: 0.9772 - val_loss: 0.0694 - val_acc: 0.9742 - val_precision: 0.9755 - val_recall: 0.9739 - val_f1: 0.9747\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.06451\n",
      "Epoch 30/100\n",
      " - 1469s - loss: 0.0620 - acc: 0.9778 - precision: 0.9783 - recall: 0.9775 - f1: 0.9779 - val_loss: 0.0620 - val_acc: 0.9781 - val_precision: 0.9783 - val_recall: 0.9778 - val_f1: 0.9781\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.06451 to 0.06205, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 31/100\n",
      " - 1471s - loss: 0.0584 - acc: 0.9791 - precision: 0.9795 - recall: 0.9786 - f1: 0.9790 - val_loss: 0.0626 - val_acc: 0.9796 - val_precision: 0.9798 - val_recall: 0.9793 - val_f1: 0.9795\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.06205\n",
      "Epoch 32/100\n",
      " - 1383s - loss: 0.0542 - acc: 0.9801 - precision: 0.9805 - recall: 0.9798 - f1: 0.9801 - val_loss: 0.1069 - val_acc: 0.9616 - val_precision: 0.9630 - val_recall: 0.9602 - val_f1: 0.9616\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.06205\n",
      "Epoch 33/100\n",
      " - 1376s - loss: 0.0562 - acc: 0.9802 - precision: 0.9807 - recall: 0.9799 - f1: 0.9803 - val_loss: 0.0579 - val_acc: 0.9800 - val_precision: 0.9809 - val_recall: 0.9792 - val_f1: 0.9800\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.06205 to 0.05793, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 34/100\n",
      " - 1378s - loss: 0.0589 - acc: 0.9794 - precision: 0.9799 - recall: 0.9789 - f1: 0.9794 - val_loss: 0.0819 - val_acc: 0.9706 - val_precision: 0.9716 - val_recall: 0.9697 - val_f1: 0.9707\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05793\n",
      "Epoch 35/100\n",
      " - 1381s - loss: 0.0554 - acc: 0.9796 - precision: 0.9800 - recall: 0.9792 - f1: 0.9796 - val_loss: 0.0921 - val_acc: 0.9664 - val_precision: 0.9675 - val_recall: 0.9660 - val_f1: 0.9667\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.05793\n",
      "Epoch 36/100\n",
      " - 1266s - loss: 0.0565 - acc: 0.9794 - precision: 0.9799 - recall: 0.9791 - f1: 0.9795 - val_loss: 0.1647 - val_acc: 0.9378 - val_precision: 0.9390 - val_recall: 0.9366 - val_f1: 0.9378\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05793\n",
      "Epoch 37/100\n",
      " - 1368s - loss: 0.0538 - acc: 0.9801 - precision: 0.9804 - recall: 0.9796 - f1: 0.9799 - val_loss: 0.0536 - val_acc: 0.9807 - val_precision: 0.9808 - val_recall: 0.9801 - val_f1: 0.9805\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.05793 to 0.05359, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 38/100\n",
      " - 1399s - loss: 0.0542 - acc: 0.9806 - precision: 0.9812 - recall: 0.9802 - f1: 0.9807 - val_loss: 0.0543 - val_acc: 0.9809 - val_precision: 0.9811 - val_recall: 0.9809 - val_f1: 0.9810\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05359\n",
      "Epoch 39/100\n",
      " - 1392s - loss: 0.0535 - acc: 0.9804 - precision: 0.9808 - recall: 0.9801 - f1: 0.9805 - val_loss: 0.0631 - val_acc: 0.9784 - val_precision: 0.9788 - val_recall: 0.9777 - val_f1: 0.9783\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.05359\n",
      "Epoch 40/100\n",
      " - 1386s - loss: 0.0549 - acc: 0.9799 - precision: 0.9805 - recall: 0.9796 - f1: 0.9800 - val_loss: 0.0526 - val_acc: 0.9826 - val_precision: 0.9827 - val_recall: 0.9822 - val_f1: 0.9824\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.05359 to 0.05265, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 41/100\n",
      " - 1373s - loss: 0.0519 - acc: 0.9821 - precision: 0.9824 - recall: 0.9817 - f1: 0.9820 - val_loss: 0.0536 - val_acc: 0.9814 - val_precision: 0.9817 - val_recall: 0.9812 - val_f1: 0.9814\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.05265\n",
      "Epoch 42/100\n",
      " - 1376s - loss: 0.0473 - acc: 0.9835 - precision: 0.9840 - recall: 0.9831 - f1: 0.9836 - val_loss: 0.0645 - val_acc: 0.9777 - val_precision: 0.9783 - val_recall: 0.9772 - val_f1: 0.9778\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.05265\n",
      "Epoch 43/100\n",
      " - 1381s - loss: 0.0502 - acc: 0.9826 - precision: 0.9829 - recall: 0.9823 - f1: 0.9826 - val_loss: 0.0428 - val_acc: 0.9867 - val_precision: 0.9869 - val_recall: 0.9866 - val_f1: 0.9867\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.05265 to 0.04279, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 44/100\n",
      " - 1379s - loss: 0.0477 - acc: 0.9827 - precision: 0.9830 - recall: 0.9824 - f1: 0.9827 - val_loss: 0.0580 - val_acc: 0.9799 - val_precision: 0.9805 - val_recall: 0.9796 - val_f1: 0.9800\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.04279\n",
      "Epoch 45/100\n",
      " - 1463s - loss: 0.0521 - acc: 0.9821 - precision: 0.9825 - recall: 0.9818 - f1: 0.9821 - val_loss: 0.0439 - val_acc: 0.9861 - val_precision: 0.9862 - val_recall: 0.9859 - val_f1: 0.9861\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.04279\n",
      "Epoch 46/100\n",
      " - 1377s - loss: 0.0477 - acc: 0.9825 - precision: 0.9829 - recall: 0.9820 - f1: 0.9824 - val_loss: 0.0484 - val_acc: 0.9839 - val_precision: 0.9842 - val_recall: 0.9834 - val_f1: 0.9838\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.04279\n",
      "Epoch 47/100\n",
      " - 1385s - loss: 0.0470 - acc: 0.9832 - precision: 0.9835 - recall: 0.9829 - f1: 0.9832 - val_loss: 0.0802 - val_acc: 0.9714 - val_precision: 0.9727 - val_recall: 0.9707 - val_f1: 0.9717\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.04279\n",
      "Epoch 48/100\n",
      " - 1369s - loss: 0.0460 - acc: 0.9840 - precision: 0.9843 - recall: 0.9837 - f1: 0.9840 - val_loss: 0.0548 - val_acc: 0.9816 - val_precision: 0.9819 - val_recall: 0.9815 - val_f1: 0.9817\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.04279\n",
      "Epoch 49/100\n",
      " - 1368s - loss: 0.0495 - acc: 0.9825 - precision: 0.9829 - recall: 0.9822 - f1: 0.9826 - val_loss: 0.0462 - val_acc: 0.9843 - val_precision: 0.9845 - val_recall: 0.9841 - val_f1: 0.9843\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.04279\n",
      "Epoch 50/100\n",
      " - 1385s - loss: 0.0465 - acc: 0.9833 - precision: 0.9838 - recall: 0.9831 - f1: 0.9835 - val_loss: 0.0430 - val_acc: 0.9855 - val_precision: 0.9860 - val_recall: 0.9851 - val_f1: 0.9855\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04279\n",
      "Epoch 51/100\n",
      " - 1382s - loss: 0.0440 - acc: 0.9842 - precision: 0.9846 - recall: 0.9840 - f1: 0.9843 - val_loss: 0.0518 - val_acc: 0.9820 - val_precision: 0.9823 - val_recall: 0.9818 - val_f1: 0.9821\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04279\n",
      "Epoch 52/100\n",
      " - 1388s - loss: 0.0454 - acc: 0.9841 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - val_loss: 0.0392 - val_acc: 0.9868 - val_precision: 0.9870 - val_recall: 0.9867 - val_f1: 0.9869\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.04279 to 0.03924, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 53/100\n",
      " - 1380s - loss: 0.0433 - acc: 0.9846 - precision: 0.9849 - recall: 0.9843 - f1: 0.9846 - val_loss: 0.0636 - val_acc: 0.9780 - val_precision: 0.9787 - val_recall: 0.9773 - val_f1: 0.9780\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03924\n",
      "Epoch 54/100\n",
      " - 1398s - loss: 0.0435 - acc: 0.9846 - precision: 0.9849 - recall: 0.9842 - f1: 0.9845 - val_loss: 0.0399 - val_acc: 0.9869 - val_precision: 0.9870 - val_recall: 0.9868 - val_f1: 0.9869\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03924\n",
      "Epoch 55/100\n",
      " - 1369s - loss: 0.0421 - acc: 0.9847 - precision: 0.9849 - recall: 0.9844 - f1: 0.9846 - val_loss: 0.0415 - val_acc: 0.9873 - val_precision: 0.9875 - val_recall: 0.9872 - val_f1: 0.9874\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03924\n",
      "Epoch 56/100\n",
      " - 1385s - loss: 0.0419 - acc: 0.9852 - precision: 0.9856 - recall: 0.9849 - f1: 0.9852 - val_loss: 0.0629 - val_acc: 0.9773 - val_precision: 0.9779 - val_recall: 0.9771 - val_f1: 0.9775\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03924\n",
      "Epoch 57/100\n",
      " - 1369s - loss: 0.0417 - acc: 0.9855 - precision: 0.9859 - recall: 0.9851 - f1: 0.9855 - val_loss: 0.0472 - val_acc: 0.9851 - val_precision: 0.9853 - val_recall: 0.9849 - val_f1: 0.9851\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03924\n",
      "Epoch 58/100\n",
      " - 1346s - loss: 0.0412 - acc: 0.9852 - precision: 0.9855 - recall: 0.9848 - f1: 0.9851 - val_loss: 0.0608 - val_acc: 0.9790 - val_precision: 0.9799 - val_recall: 0.9786 - val_f1: 0.9792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03924\n",
      "Epoch 59/100\n",
      " - 1321s - loss: 0.0410 - acc: 0.9859 - precision: 0.9861 - recall: 0.9856 - f1: 0.9858 - val_loss: 0.0348 - val_acc: 0.9885 - val_precision: 0.9886 - val_recall: 0.9884 - val_f1: 0.9885\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03924 to 0.03478, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 60/100\n",
      " - 1363s - loss: 0.0427 - acc: 0.9851 - precision: 0.9855 - recall: 0.9849 - f1: 0.9852 - val_loss: 0.0400 - val_acc: 0.9877 - val_precision: 0.9880 - val_recall: 0.9876 - val_f1: 0.9878\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03478\n",
      "Epoch 61/100\n",
      " - 1377s - loss: 0.0400 - acc: 0.9860 - precision: 0.9863 - recall: 0.9858 - f1: 0.9861 - val_loss: 0.0573 - val_acc: 0.9819 - val_precision: 0.9822 - val_recall: 0.9818 - val_f1: 0.9820\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03478\n",
      "Epoch 62/100\n",
      " - 1422s - loss: 0.0388 - acc: 0.9867 - precision: 0.9869 - recall: 0.9864 - f1: 0.9867 - val_loss: 0.1322 - val_acc: 0.9503 - val_precision: 0.9515 - val_recall: 0.9494 - val_f1: 0.9504\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03478\n",
      "Epoch 63/100\n",
      " - 1404s - loss: 0.0371 - acc: 0.9868 - precision: 0.9870 - recall: 0.9865 - f1: 0.9868 - val_loss: 0.0783 - val_acc: 0.9716 - val_precision: 0.9723 - val_recall: 0.9713 - val_f1: 0.9718\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03478\n",
      "Epoch 64/100\n",
      " - 1418s - loss: 0.0388 - acc: 0.9861 - precision: 0.9864 - recall: 0.9859 - f1: 0.9861 - val_loss: 0.0361 - val_acc: 0.9887 - val_precision: 0.9888 - val_recall: 0.9887 - val_f1: 0.9887\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03478\n",
      "Epoch 65/100\n",
      " - 1413s - loss: 0.0381 - acc: 0.9868 - precision: 0.9871 - recall: 0.9866 - f1: 0.9869 - val_loss: 0.0346 - val_acc: 0.9878 - val_precision: 0.9879 - val_recall: 0.9878 - val_f1: 0.9879\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.03478 to 0.03461, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 66/100\n",
      " - 1347s - loss: 0.0395 - acc: 0.9862 - precision: 0.9864 - recall: 0.9859 - f1: 0.9862 - val_loss: 0.0385 - val_acc: 0.9875 - val_precision: 0.9877 - val_recall: 0.9875 - val_f1: 0.9876\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03461\n",
      "Epoch 67/100\n",
      " - 1363s - loss: 0.0400 - acc: 0.9862 - precision: 0.9865 - recall: 0.9859 - f1: 0.9862 - val_loss: 0.0703 - val_acc: 0.9762 - val_precision: 0.9766 - val_recall: 0.9756 - val_f1: 0.9761\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03461\n",
      "Epoch 68/100\n",
      " - 1370s - loss: 0.0388 - acc: 0.9863 - precision: 0.9865 - recall: 0.9860 - f1: 0.9862 - val_loss: 0.0407 - val_acc: 0.9875 - val_precision: 0.9877 - val_recall: 0.9874 - val_f1: 0.9875\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03461\n",
      "Epoch 69/100\n",
      " - 1356s - loss: 0.0365 - acc: 0.9875 - precision: 0.9877 - recall: 0.9873 - f1: 0.9875 - val_loss: 0.0438 - val_acc: 0.9859 - val_precision: 0.9863 - val_recall: 0.9858 - val_f1: 0.9860\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03461\n",
      "Epoch 70/100\n",
      " - 1421s - loss: 0.0365 - acc: 0.9871 - precision: 0.9873 - recall: 0.9868 - f1: 0.9871 - val_loss: 0.0389 - val_acc: 0.9876 - val_precision: 0.9876 - val_recall: 0.9873 - val_f1: 0.9875\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03461\n",
      "Epoch 71/100\n",
      " - 1418s - loss: 0.0380 - acc: 0.9868 - precision: 0.9871 - recall: 0.9866 - f1: 0.9868 - val_loss: 0.0669 - val_acc: 0.9768 - val_precision: 0.9771 - val_recall: 0.9763 - val_f1: 0.9767\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03461\n",
      "Epoch 72/100\n",
      " - 1398s - loss: 0.0360 - acc: 0.9870 - precision: 0.9872 - recall: 0.9867 - f1: 0.9870 - val_loss: 0.0555 - val_acc: 0.9820 - val_precision: 0.9820 - val_recall: 0.9818 - val_f1: 0.9819\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03461\n",
      "Epoch 73/100\n",
      " - 1390s - loss: 0.0373 - acc: 0.9872 - precision: 0.9875 - recall: 0.9870 - f1: 0.9872 - val_loss: 0.0363 - val_acc: 0.9884 - val_precision: 0.9884 - val_recall: 0.9883 - val_f1: 0.9883\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03461\n",
      "Epoch 74/100\n",
      " - 1264s - loss: 0.0363 - acc: 0.9875 - precision: 0.9878 - recall: 0.9872 - f1: 0.9875 - val_loss: 0.0336 - val_acc: 0.9895 - val_precision: 0.9898 - val_recall: 0.9894 - val_f1: 0.9896\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.03461 to 0.03359, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 75/100\n",
      " - 1408s - loss: 0.0346 - acc: 0.9880 - precision: 0.9883 - recall: 0.9878 - f1: 0.9880 - val_loss: 0.0460 - val_acc: 0.9865 - val_precision: 0.9865 - val_recall: 0.9863 - val_f1: 0.9864\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03359\n",
      "Epoch 76/100\n",
      " - 1409s - loss: 0.0358 - acc: 0.9876 - precision: 0.9878 - recall: 0.9874 - f1: 0.9876 - val_loss: 0.0362 - val_acc: 0.9880 - val_precision: 0.9881 - val_recall: 0.9877 - val_f1: 0.9879\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03359\n",
      "Epoch 77/100\n",
      " - 1407s - loss: 0.0363 - acc: 0.9876 - precision: 0.9878 - recall: 0.9873 - f1: 0.9875 - val_loss: 0.0411 - val_acc: 0.9866 - val_precision: 0.9868 - val_recall: 0.9866 - val_f1: 0.9867\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03359\n",
      "Epoch 78/100\n",
      " - 1377s - loss: 0.0363 - acc: 0.9872 - precision: 0.9874 - recall: 0.9869 - f1: 0.9872 - val_loss: 0.0389 - val_acc: 0.9879 - val_precision: 0.9881 - val_recall: 0.9878 - val_f1: 0.9880\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03359\n",
      "Epoch 79/100\n",
      " - 1411s - loss: 0.0356 - acc: 0.9870 - precision: 0.9872 - recall: 0.9868 - f1: 0.9870 - val_loss: 0.0399 - val_acc: 0.9871 - val_precision: 0.9872 - val_recall: 0.9871 - val_f1: 0.9871\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03359\n",
      "Epoch 80/100\n",
      " - 1407s - loss: 0.0369 - acc: 0.9869 - precision: 0.9871 - recall: 0.9867 - f1: 0.9869 - val_loss: 0.0436 - val_acc: 0.9858 - val_precision: 0.9861 - val_recall: 0.9855 - val_f1: 0.9858\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03359\n",
      "Epoch 81/100\n",
      " - 1445s - loss: 0.0353 - acc: 0.9873 - precision: 0.9875 - recall: 0.9871 - f1: 0.9873 - val_loss: 0.0517 - val_acc: 0.9831 - val_precision: 0.9839 - val_recall: 0.9827 - val_f1: 0.9833\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03359\n",
      "Epoch 82/100\n",
      " - 1436s - loss: 0.0325 - acc: 0.9888 - precision: 0.9890 - recall: 0.9885 - f1: 0.9888 - val_loss: 0.0501 - val_acc: 0.9845 - val_precision: 0.9848 - val_recall: 0.9844 - val_f1: 0.9846\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03359\n",
      "Epoch 83/100\n",
      " - 1505s - loss: 0.0327 - acc: 0.9886 - precision: 0.9888 - recall: 0.9884 - f1: 0.9886 - val_loss: 0.0381 - val_acc: 0.9870 - val_precision: 0.9874 - val_recall: 0.9869 - val_f1: 0.9871\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03359\n",
      "Epoch 84/100\n",
      " - 1447s - loss: 0.0323 - acc: 0.9890 - precision: 0.9891 - recall: 0.9888 - f1: 0.9889 - val_loss: 0.0354 - val_acc: 0.9897 - val_precision: 0.9897 - val_recall: 0.9896 - val_f1: 0.9897\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03359\n",
      "Epoch 85/100\n",
      " - 1394s - loss: 0.0306 - acc: 0.9891 - precision: 0.9894 - recall: 0.9890 - f1: 0.9892 - val_loss: 0.0355 - val_acc: 0.9894 - val_precision: 0.9894 - val_recall: 0.9893 - val_f1: 0.9893\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03359\n",
      "Epoch 86/100\n",
      " - 1462s - loss: 0.0328 - acc: 0.9883 - precision: 0.9887 - recall: 0.9882 - f1: 0.9884 - val_loss: 0.0426 - val_acc: 0.9866 - val_precision: 0.9869 - val_recall: 0.9866 - val_f1: 0.9867\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03359\n",
      "Epoch 87/100\n",
      " - 1450s - loss: 0.0329 - acc: 0.9889 - precision: 0.9890 - recall: 0.9888 - f1: 0.9889 - val_loss: 0.0317 - val_acc: 0.9896 - val_precision: 0.9900 - val_recall: 0.9895 - val_f1: 0.9897\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.03359 to 0.03171, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 88/100\n",
      " - 1374s - loss: 0.0342 - acc: 0.9876 - precision: 0.9879 - recall: 0.9874 - f1: 0.9876 - val_loss: 0.0540 - val_acc: 0.9832 - val_precision: 0.9835 - val_recall: 0.9829 - val_f1: 0.9832\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03171\n",
      "Epoch 89/100\n",
      " - 1382s - loss: 0.0321 - acc: 0.9886 - precision: 0.9888 - recall: 0.9884 - f1: 0.9886 - val_loss: 0.0344 - val_acc: 0.9886 - val_precision: 0.9886 - val_recall: 0.9886 - val_f1: 0.9886\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03171\n",
      "Epoch 90/100\n",
      " - 1388s - loss: 0.0326 - acc: 0.9886 - precision: 0.9887 - recall: 0.9883 - f1: 0.9885 - val_loss: 0.0341 - val_acc: 0.9904 - val_precision: 0.9905 - val_recall: 0.9904 - val_f1: 0.9904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03171\n",
      "Epoch 91/100\n",
      " - 1368s - loss: 0.0310 - acc: 0.9892 - precision: 0.9894 - recall: 0.9890 - f1: 0.9892 - val_loss: 0.0268 - val_acc: 0.9920 - val_precision: 0.9921 - val_recall: 0.9919 - val_f1: 0.9920\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.03171 to 0.02679, saving model to models/heysaw_fold_1.h5\n",
      "Epoch 92/100\n",
      " - 1363s - loss: 0.0326 - acc: 0.9884 - precision: 0.9886 - recall: 0.9882 - f1: 0.9884 - val_loss: 0.0416 - val_acc: 0.9877 - val_precision: 0.9878 - val_recall: 0.9875 - val_f1: 0.9876\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02679\n",
      "Epoch 93/100\n",
      " - 1401s - loss: 0.0306 - acc: 0.9894 - precision: 0.9895 - recall: 0.9893 - f1: 0.9894 - val_loss: 0.0322 - val_acc: 0.9894 - val_precision: 0.9896 - val_recall: 0.9890 - val_f1: 0.9893\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02679\n",
      "Epoch 94/100\n",
      " - 1376s - loss: 0.0308 - acc: 0.9891 - precision: 0.9893 - recall: 0.9889 - f1: 0.9891 - val_loss: 0.0336 - val_acc: 0.9892 - val_precision: 0.9893 - val_recall: 0.9891 - val_f1: 0.9892\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02679\n",
      "Epoch 95/100\n",
      " - 1371s - loss: 0.0303 - acc: 0.9895 - precision: 0.9898 - recall: 0.9894 - f1: 0.9896 - val_loss: 0.0332 - val_acc: 0.9899 - val_precision: 0.9903 - val_recall: 0.9898 - val_f1: 0.9900\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02679\n",
      "Epoch 96/100\n",
      " - 1340s - loss: 0.0316 - acc: 0.9890 - precision: 0.9893 - recall: 0.9888 - f1: 0.9890 - val_loss: 0.0372 - val_acc: 0.9892 - val_precision: 0.9892 - val_recall: 0.9891 - val_f1: 0.9892\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02679\n",
      "Epoch 97/100\n",
      " - 1385s - loss: 0.0314 - acc: 0.9890 - precision: 0.9893 - recall: 0.9889 - f1: 0.9891 - val_loss: 0.0339 - val_acc: 0.9905 - val_precision: 0.9906 - val_recall: 0.9905 - val_f1: 0.9905\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02679\n",
      "Epoch 98/100\n",
      " - 1381s - loss: 0.0295 - acc: 0.9897 - precision: 0.9899 - recall: 0.9894 - f1: 0.9897 - val_loss: 0.0422 - val_acc: 0.9876 - val_precision: 0.9876 - val_recall: 0.9876 - val_f1: 0.9876\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02679\n",
      "Epoch 99/100\n",
      " - 1393s - loss: 0.0292 - acc: 0.9901 - precision: 0.9902 - recall: 0.9898 - f1: 0.9900 - val_loss: 0.0394 - val_acc: 0.9864 - val_precision: 0.9865 - val_recall: 0.9862 - val_f1: 0.9864\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02679\n",
      "Epoch 100/100\n",
      " - 1367s - loss: 0.0301 - acc: 0.9895 - precision: 0.9898 - recall: 0.9893 - f1: 0.9895 - val_loss: 0.0313 - val_acc: 0.9910 - val_precision: 0.9912 - val_recall: 0.9909 - val_f1: 0.9910\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02679\n",
      "** FOLD 2 **\n",
      "Epoch 1/100\n",
      " - 1678s - loss: 0.0892 - acc: 0.9692 - precision: 0.9698 - recall: 0.9686 - f1: 0.9692 - val_loss: 0.1943 - val_acc: 0.9329 - val_precision: 0.9357 - val_recall: 0.9313 - val_f1: 0.9334\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19425, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 2/100\n",
      " - 1369s - loss: 0.0793 - acc: 0.9712 - precision: 0.9717 - recall: 0.9709 - f1: 0.9713 - val_loss: 0.1102 - val_acc: 0.9627 - val_precision: 0.9636 - val_recall: 0.9620 - val_f1: 0.9628\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19425 to 0.11023, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 3/100\n",
      " - 1256s - loss: 0.0725 - acc: 0.9739 - precision: 0.9744 - recall: 0.9735 - f1: 0.9739 - val_loss: 0.3214 - val_acc: 0.8963 - val_precision: 0.8997 - val_recall: 0.8946 - val_f1: 0.8971\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11023\n",
      "Epoch 4/100\n",
      " - 1235s - loss: 0.0778 - acc: 0.9733 - precision: 0.9738 - recall: 0.9729 - f1: 0.9733 - val_loss: 0.3227 - val_acc: 0.8961 - val_precision: 0.8977 - val_recall: 0.8950 - val_f1: 0.8963\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11023\n",
      "Epoch 5/100\n",
      " - 1276s - loss: 0.0696 - acc: 0.9750 - precision: 0.9756 - recall: 0.9745 - f1: 0.9750 - val_loss: 0.0911 - val_acc: 0.9664 - val_precision: 0.9673 - val_recall: 0.9658 - val_f1: 0.9665\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11023 to 0.09105, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 6/100\n",
      " - 1248s - loss: 0.0720 - acc: 0.9738 - precision: 0.9745 - recall: 0.9735 - f1: 0.9740 - val_loss: 0.2752 - val_acc: 0.9119 - val_precision: 0.9173 - val_recall: 0.9085 - val_f1: 0.9128\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09105\n",
      "Epoch 7/100\n",
      " - 1254s - loss: 0.0726 - acc: 0.9742 - precision: 0.9749 - recall: 0.9738 - f1: 0.9743 - val_loss: 0.1994 - val_acc: 0.9257 - val_precision: 0.9277 - val_recall: 0.9234 - val_f1: 0.9255\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09105\n",
      "Epoch 8/100\n",
      " - 1254s - loss: 0.0683 - acc: 0.9760 - precision: 0.9766 - recall: 0.9754 - f1: 0.9760 - val_loss: 0.1770 - val_acc: 0.9308 - val_precision: 0.9320 - val_recall: 0.9297 - val_f1: 0.9308\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09105\n",
      "Epoch 9/100\n",
      " - 1248s - loss: 0.0706 - acc: 0.9744 - precision: 0.9751 - recall: 0.9738 - f1: 0.9744 - val_loss: 0.0661 - val_acc: 0.9758 - val_precision: 0.9767 - val_recall: 0.9752 - val_f1: 0.9759\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09105 to 0.06608, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 10/100\n",
      " - 1259s - loss: 0.0674 - acc: 0.9754 - precision: 0.9759 - recall: 0.9748 - f1: 0.9753 - val_loss: 0.0560 - val_acc: 0.9793 - val_precision: 0.9798 - val_recall: 0.9791 - val_f1: 0.9794\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06608 to 0.05597, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 11/100\n",
      " - 1242s - loss: 0.0610 - acc: 0.9784 - precision: 0.9790 - recall: 0.9778 - f1: 0.9784 - val_loss: 0.0486 - val_acc: 0.9831 - val_precision: 0.9832 - val_recall: 0.9826 - val_f1: 0.9829\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05597 to 0.04862, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 12/100\n",
      " - 1236s - loss: 0.0575 - acc: 0.9791 - precision: 0.9795 - recall: 0.9787 - f1: 0.9791 - val_loss: 0.0643 - val_acc: 0.9773 - val_precision: 0.9778 - val_recall: 0.9767 - val_f1: 0.9773\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04862\n",
      "Epoch 13/100\n",
      " - 1263s - loss: 0.0570 - acc: 0.9795 - precision: 0.9798 - recall: 0.9791 - f1: 0.9794 - val_loss: 0.0513 - val_acc: 0.9825 - val_precision: 0.9828 - val_recall: 0.9821 - val_f1: 0.9825\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04862\n",
      "Epoch 14/100\n",
      " - 1243s - loss: 0.0556 - acc: 0.9805 - precision: 0.9808 - recall: 0.9801 - f1: 0.9805 - val_loss: 0.0495 - val_acc: 0.9829 - val_precision: 0.9834 - val_recall: 0.9826 - val_f1: 0.9830\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04862\n",
      "Epoch 15/100\n",
      " - 1213s - loss: 0.0579 - acc: 0.9789 - precision: 0.9792 - recall: 0.9785 - f1: 0.9789 - val_loss: 0.1418 - val_acc: 0.9519 - val_precision: 0.9536 - val_recall: 0.9508 - val_f1: 0.9522\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04862\n",
      "Epoch 16/100\n",
      " - 1247s - loss: 0.0550 - acc: 0.9805 - precision: 0.9808 - recall: 0.9800 - f1: 0.9804 - val_loss: 0.1622 - val_acc: 0.9382 - val_precision: 0.9411 - val_recall: 0.9368 - val_f1: 0.9389\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04862\n",
      "Epoch 17/100\n",
      " - 1351s - loss: 0.0546 - acc: 0.9805 - precision: 0.9808 - recall: 0.9802 - f1: 0.9805 - val_loss: 0.0547 - val_acc: 0.9802 - val_precision: 0.9803 - val_recall: 0.9799 - val_f1: 0.9801\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04862\n",
      "Epoch 18/100\n",
      " - 1400s - loss: 0.0551 - acc: 0.9799 - precision: 0.9803 - recall: 0.9795 - f1: 0.9799 - val_loss: 0.3640 - val_acc: 0.9155 - val_precision: 0.9161 - val_recall: 0.9151 - val_f1: 0.9156\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04862\n",
      "Epoch 19/100\n",
      " - 1207s - loss: 0.0563 - acc: 0.9798 - precision: 0.9801 - recall: 0.9794 - f1: 0.9798 - val_loss: 0.0359 - val_acc: 0.9874 - val_precision: 0.9876 - val_recall: 0.9872 - val_f1: 0.9874\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.04862 to 0.03594, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 20/100\n",
      " - 1260s - loss: 0.0532 - acc: 0.9812 - precision: 0.9817 - recall: 0.9809 - f1: 0.9813 - val_loss: 0.1795 - val_acc: 0.9319 - val_precision: 0.9340 - val_recall: 0.9286 - val_f1: 0.9312\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03594\n",
      "Epoch 21/100\n",
      " - 2071s - loss: 0.0520 - acc: 0.9810 - precision: 0.9813 - recall: 0.9807 - f1: 0.9810 - val_loss: 0.2067 - val_acc: 0.9268 - val_precision: 0.9300 - val_recall: 0.9253 - val_f1: 0.9276\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03594\n",
      "Epoch 22/100\n",
      " - 1870s - loss: 0.0469 - acc: 0.9830 - precision: 0.9833 - recall: 0.9828 - f1: 0.9831 - val_loss: 0.0384 - val_acc: 0.9885 - val_precision: 0.9890 - val_recall: 0.9879 - val_f1: 0.9885\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03594\n",
      "Epoch 23/100\n",
      " - 1470s - loss: 0.0474 - acc: 0.9828 - precision: 0.9830 - recall: 0.9825 - f1: 0.9828 - val_loss: 0.0428 - val_acc: 0.9838 - val_precision: 0.9838 - val_recall: 0.9838 - val_f1: 0.9838\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03594\n",
      "Epoch 24/100\n",
      " - 1653s - loss: 0.0463 - acc: 0.9835 - precision: 0.9839 - recall: 0.9832 - f1: 0.9836 - val_loss: 0.0494 - val_acc: 0.9836 - val_precision: 0.9839 - val_recall: 0.9835 - val_f1: 0.9837\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03594\n",
      "Epoch 25/100\n",
      " - 1285s - loss: 0.0442 - acc: 0.9836 - precision: 0.9840 - recall: 0.9833 - f1: 0.9836 - val_loss: 0.0459 - val_acc: 0.9827 - val_precision: 0.9832 - val_recall: 0.9827 - val_f1: 0.9829\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03594\n",
      "Epoch 26/100\n",
      " - 1239s - loss: 0.0441 - acc: 0.9845 - precision: 0.9847 - recall: 0.9842 - f1: 0.9844 - val_loss: 0.0307 - val_acc: 0.9896 - val_precision: 0.9897 - val_recall: 0.9896 - val_f1: 0.9896\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.03594 to 0.03071, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 27/100\n",
      " - 1391s - loss: 0.0475 - acc: 0.9826 - precision: 0.9830 - recall: 0.9823 - f1: 0.9826 - val_loss: 0.0772 - val_acc: 0.9723 - val_precision: 0.9727 - val_recall: 0.9721 - val_f1: 0.9724\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03071\n",
      "Epoch 28/100\n",
      " - 1311s - loss: 0.0428 - acc: 0.9852 - precision: 0.9854 - recall: 0.9849 - f1: 0.9851 - val_loss: 0.0744 - val_acc: 0.9721 - val_precision: 0.9732 - val_recall: 0.9713 - val_f1: 0.9723\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03071\n",
      "Epoch 29/100\n",
      " - 1221s - loss: 0.0440 - acc: 0.9843 - precision: 0.9846 - recall: 0.9838 - f1: 0.9842 - val_loss: 0.0560 - val_acc: 0.9792 - val_precision: 0.9795 - val_recall: 0.9792 - val_f1: 0.9793\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03071\n",
      "Epoch 30/100\n",
      " - 1389s - loss: 0.0464 - acc: 0.9838 - precision: 0.9841 - recall: 0.9835 - f1: 0.9838 - val_loss: 0.0394 - val_acc: 0.9858 - val_precision: 0.9863 - val_recall: 0.9856 - val_f1: 0.9859\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03071\n",
      "Epoch 31/100\n",
      " - 1259s - loss: 0.0430 - acc: 0.9843 - precision: 0.9847 - recall: 0.9840 - f1: 0.9843 - val_loss: 0.0427 - val_acc: 0.9854 - val_precision: 0.9855 - val_recall: 0.9854 - val_f1: 0.9855\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03071\n",
      "Epoch 32/100\n",
      " - 1271s - loss: 0.0402 - acc: 0.9859 - precision: 0.9862 - recall: 0.9856 - f1: 0.9859 - val_loss: 0.0802 - val_acc: 0.9712 - val_precision: 0.9717 - val_recall: 0.9707 - val_f1: 0.9712\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03071\n",
      "Epoch 33/100\n",
      " - 1252s - loss: 0.0396 - acc: 0.9855 - precision: 0.9858 - recall: 0.9852 - f1: 0.9855 - val_loss: 0.0594 - val_acc: 0.9784 - val_precision: 0.9788 - val_recall: 0.9780 - val_f1: 0.9784\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03071\n",
      "Epoch 34/100\n",
      " - 1375s - loss: 0.0394 - acc: 0.9859 - precision: 0.9863 - recall: 0.9856 - f1: 0.9859 - val_loss: 0.1519 - val_acc: 0.9514 - val_precision: 0.9518 - val_recall: 0.9508 - val_f1: 0.9513\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.03071\n",
      "Epoch 35/100\n",
      " - 1330s - loss: 0.0418 - acc: 0.9852 - precision: 0.9855 - recall: 0.9849 - f1: 0.9852 - val_loss: 0.0465 - val_acc: 0.9832 - val_precision: 0.9836 - val_recall: 0.9829 - val_f1: 0.9832\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03071\n",
      "Epoch 36/100\n",
      " - 1312s - loss: 0.0395 - acc: 0.9860 - precision: 0.9862 - recall: 0.9857 - f1: 0.9859 - val_loss: 0.0863 - val_acc: 0.9721 - val_precision: 0.9724 - val_recall: 0.9718 - val_f1: 0.9721\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.03071\n",
      "Epoch 37/100\n",
      " - 1328s - loss: 0.0388 - acc: 0.9861 - precision: 0.9864 - recall: 0.9859 - f1: 0.9861 - val_loss: 0.0365 - val_acc: 0.9869 - val_precision: 0.9872 - val_recall: 0.9868 - val_f1: 0.9870\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03071\n",
      "Epoch 38/100\n",
      " - 1303s - loss: 0.0405 - acc: 0.9856 - precision: 0.9860 - recall: 0.9854 - f1: 0.9857 - val_loss: 0.1366 - val_acc: 0.9582 - val_precision: 0.9588 - val_recall: 0.9575 - val_f1: 0.9581\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03071\n",
      "Epoch 39/100\n",
      " - 1270s - loss: 0.0376 - acc: 0.9868 - precision: 0.9870 - recall: 0.9867 - f1: 0.9868 - val_loss: 0.0301 - val_acc: 0.9906 - val_precision: 0.9906 - val_recall: 0.9904 - val_f1: 0.9905\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.03071 to 0.03010, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 40/100\n",
      " - 1270s - loss: 0.0370 - acc: 0.9868 - precision: 0.9871 - recall: 0.9867 - f1: 0.9869 - val_loss: 0.0302 - val_acc: 0.9902 - val_precision: 0.9905 - val_recall: 0.9901 - val_f1: 0.9903\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03010\n",
      "Epoch 41/100\n",
      " - 1346s - loss: 0.0379 - acc: 0.9864 - precision: 0.9867 - recall: 0.9862 - f1: 0.9865 - val_loss: 0.1287 - val_acc: 0.9574 - val_precision: 0.9582 - val_recall: 0.9569 - val_f1: 0.9576\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03010\n",
      "Epoch 42/100\n",
      " - 1273s - loss: 0.0373 - acc: 0.9867 - precision: 0.9871 - recall: 0.9866 - f1: 0.9868 - val_loss: 0.0366 - val_acc: 0.9867 - val_precision: 0.9870 - val_recall: 0.9865 - val_f1: 0.9867\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03010\n",
      "Epoch 43/100\n",
      " - 1275s - loss: 0.0353 - acc: 0.9870 - precision: 0.9872 - recall: 0.9869 - f1: 0.9870 - val_loss: 0.0403 - val_acc: 0.9850 - val_precision: 0.9854 - val_recall: 0.9849 - val_f1: 0.9852\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03010\n",
      "Epoch 44/100\n",
      " - 1291s - loss: 0.0363 - acc: 0.9869 - precision: 0.9871 - recall: 0.9868 - f1: 0.9869 - val_loss: 0.0288 - val_acc: 0.9896 - val_precision: 0.9901 - val_recall: 0.9892 - val_f1: 0.9896\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03010 to 0.02884, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 45/100\n",
      " - 1346s - loss: 0.0349 - acc: 0.9879 - precision: 0.9880 - recall: 0.9877 - f1: 0.9878 - val_loss: 0.0789 - val_acc: 0.9697 - val_precision: 0.9702 - val_recall: 0.9693 - val_f1: 0.9697\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02884\n",
      "Epoch 46/100\n",
      " - 1333s - loss: 0.0352 - acc: 0.9881 - precision: 0.9882 - recall: 0.9879 - f1: 0.9881 - val_loss: 0.0500 - val_acc: 0.9814 - val_precision: 0.9814 - val_recall: 0.9811 - val_f1: 0.9813\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02884\n",
      "Epoch 47/100\n",
      " - 1272s - loss: 0.0324 - acc: 0.9885 - precision: 0.9888 - recall: 0.9883 - f1: 0.9885 - val_loss: 0.0299 - val_acc: 0.9910 - val_precision: 0.9912 - val_recall: 0.9909 - val_f1: 0.9910\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02884\n",
      "Epoch 48/100\n",
      " - 1265s - loss: 0.0343 - acc: 0.9873 - precision: 0.9876 - recall: 0.9872 - f1: 0.9874 - val_loss: 0.0464 - val_acc: 0.9847 - val_precision: 0.9849 - val_recall: 0.9847 - val_f1: 0.9848\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02884\n",
      "Epoch 49/100\n",
      " - 1292s - loss: 0.0325 - acc: 0.9880 - precision: 0.9883 - recall: 0.9878 - f1: 0.9881 - val_loss: 0.0379 - val_acc: 0.9863 - val_precision: 0.9865 - val_recall: 0.9862 - val_f1: 0.9864\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02884\n",
      "Epoch 50/100\n",
      " - 1266s - loss: 0.0333 - acc: 0.9881 - precision: 0.9883 - recall: 0.9880 - f1: 0.9881 - val_loss: 0.0380 - val_acc: 0.9876 - val_precision: 0.9883 - val_recall: 0.9873 - val_f1: 0.9878\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02884\n",
      "Epoch 51/100\n",
      " - 1262s - loss: 0.0313 - acc: 0.9888 - precision: 0.9890 - recall: 0.9886 - f1: 0.9888 - val_loss: 0.0246 - val_acc: 0.9921 - val_precision: 0.9922 - val_recall: 0.9918 - val_f1: 0.9920\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.02884 to 0.02464, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 52/100\n",
      " - 1244s - loss: 0.0327 - acc: 0.9890 - precision: 0.9893 - recall: 0.9888 - f1: 0.9891 - val_loss: 0.0372 - val_acc: 0.9878 - val_precision: 0.9880 - val_recall: 0.9877 - val_f1: 0.9878\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02464\n",
      "Epoch 53/100\n",
      " - 1259s - loss: 0.0315 - acc: 0.9889 - precision: 0.9890 - recall: 0.9888 - f1: 0.9889 - val_loss: 0.0254 - val_acc: 0.9911 - val_precision: 0.9912 - val_recall: 0.9910 - val_f1: 0.9911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02464\n",
      "Epoch 54/100\n",
      " - 1218s - loss: 0.0314 - acc: 0.9892 - precision: 0.9894 - recall: 0.9891 - f1: 0.9892 - val_loss: 0.0344 - val_acc: 0.9875 - val_precision: 0.9879 - val_recall: 0.9873 - val_f1: 0.9876\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02464\n",
      "Epoch 55/100\n",
      " - 1331s - loss: 0.0310 - acc: 0.9894 - precision: 0.9895 - recall: 0.9892 - f1: 0.9894 - val_loss: 0.0358 - val_acc: 0.9881 - val_precision: 0.9882 - val_recall: 0.9879 - val_f1: 0.9880\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02464\n",
      "Epoch 56/100\n",
      " - 1314s - loss: 0.0296 - acc: 0.9895 - precision: 0.9897 - recall: 0.9893 - f1: 0.9895 - val_loss: 0.0262 - val_acc: 0.9917 - val_precision: 0.9917 - val_recall: 0.9916 - val_f1: 0.9916\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02464\n",
      "Epoch 57/100\n",
      " - 1297s - loss: 0.0284 - acc: 0.9899 - precision: 0.9901 - recall: 0.9897 - f1: 0.9899 - val_loss: 0.0224 - val_acc: 0.9928 - val_precision: 0.9928 - val_recall: 0.9926 - val_f1: 0.9927\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02464 to 0.02235, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 58/100\n",
      " - 1415s - loss: 0.0325 - acc: 0.9886 - precision: 0.9888 - recall: 0.9885 - f1: 0.9886 - val_loss: 0.0369 - val_acc: 0.9879 - val_precision: 0.9880 - val_recall: 0.9878 - val_f1: 0.9879\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02235\n",
      "Epoch 59/100\n",
      " - 1334s - loss: 0.0297 - acc: 0.9901 - precision: 0.9903 - recall: 0.9899 - f1: 0.9901 - val_loss: 0.0306 - val_acc: 0.9907 - val_precision: 0.9908 - val_recall: 0.9907 - val_f1: 0.9907\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02235\n",
      "Epoch 60/100\n",
      " - 1319s - loss: 0.0292 - acc: 0.9900 - precision: 0.9901 - recall: 0.9898 - f1: 0.9899 - val_loss: 0.0292 - val_acc: 0.9909 - val_precision: 0.9909 - val_recall: 0.9908 - val_f1: 0.9909\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02235\n",
      "Epoch 61/100\n",
      " - 1305s - loss: 0.0287 - acc: 0.9906 - precision: 0.9909 - recall: 0.9904 - f1: 0.9906 - val_loss: 0.0209 - val_acc: 0.9932 - val_precision: 0.9932 - val_recall: 0.9931 - val_f1: 0.9932\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02235 to 0.02090, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 62/100\n",
      " - 1305s - loss: 0.0266 - acc: 0.9904 - precision: 0.9906 - recall: 0.9903 - f1: 0.9904 - val_loss: 0.0361 - val_acc: 0.9886 - val_precision: 0.9887 - val_recall: 0.9885 - val_f1: 0.9886\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02090\n",
      "Epoch 63/100\n",
      " - 1217s - loss: 0.0289 - acc: 0.9901 - precision: 0.9902 - recall: 0.9900 - f1: 0.9901 - val_loss: 0.0279 - val_acc: 0.9912 - val_precision: 0.9912 - val_recall: 0.9910 - val_f1: 0.9911\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02090\n",
      "Epoch 64/100\n",
      " - 1262s - loss: 0.0275 - acc: 0.9907 - precision: 0.9908 - recall: 0.9905 - f1: 0.9907 - val_loss: 0.0241 - val_acc: 0.9920 - val_precision: 0.9921 - val_recall: 0.9918 - val_f1: 0.9920\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02090\n",
      "Epoch 65/100\n",
      " - 1282s - loss: 0.0283 - acc: 0.9903 - precision: 0.9905 - recall: 0.9902 - f1: 0.9904 - val_loss: 0.0213 - val_acc: 0.9927 - val_precision: 0.9929 - val_recall: 0.9927 - val_f1: 0.9928\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02090\n",
      "Epoch 66/100\n",
      " - 1388s - loss: 0.0271 - acc: 0.9908 - precision: 0.9909 - recall: 0.9907 - f1: 0.9908 - val_loss: 0.0263 - val_acc: 0.9925 - val_precision: 0.9924 - val_recall: 0.9923 - val_f1: 0.9924\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02090\n",
      "Epoch 67/100\n",
      " - 1254s - loss: 0.0298 - acc: 0.9894 - precision: 0.9896 - recall: 0.9893 - f1: 0.9894 - val_loss: 0.0280 - val_acc: 0.9918 - val_precision: 0.9919 - val_recall: 0.9917 - val_f1: 0.9918\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02090\n",
      "Epoch 68/100\n",
      " - 1318s - loss: 0.0289 - acc: 0.9902 - precision: 0.9902 - recall: 0.9901 - f1: 0.9902 - val_loss: 0.0207 - val_acc: 0.9937 - val_precision: 0.9937 - val_recall: 0.9936 - val_f1: 0.9937\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.02090 to 0.02074, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 69/100\n",
      " - 1341s - loss: 0.0270 - acc: 0.9906 - precision: 0.9907 - recall: 0.9904 - f1: 0.9906 - val_loss: 0.0229 - val_acc: 0.9933 - val_precision: 0.9934 - val_recall: 0.9933 - val_f1: 0.9933\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02074\n",
      "Epoch 70/100\n",
      " - 1332s - loss: 0.0289 - acc: 0.9899 - precision: 0.9901 - recall: 0.9898 - f1: 0.9899 - val_loss: 0.0253 - val_acc: 0.9922 - val_precision: 0.9922 - val_recall: 0.9921 - val_f1: 0.9921\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02074\n",
      "Epoch 71/100\n",
      " - 1322s - loss: 0.0269 - acc: 0.9908 - precision: 0.9909 - recall: 0.9907 - f1: 0.9908 - val_loss: 0.0293 - val_acc: 0.9903 - val_precision: 0.9905 - val_recall: 0.9902 - val_f1: 0.9904\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02074\n",
      "Epoch 72/100\n",
      " - 1404s - loss: 0.0271 - acc: 0.9908 - precision: 0.9910 - recall: 0.9907 - f1: 0.9908 - val_loss: 0.0213 - val_acc: 0.9941 - val_precision: 0.9941 - val_recall: 0.9940 - val_f1: 0.9941\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02074\n",
      "Epoch 73/100\n",
      " - 1381s - loss: 0.0247 - acc: 0.9917 - precision: 0.9918 - recall: 0.9915 - f1: 0.9916 - val_loss: 0.0272 - val_acc: 0.9917 - val_precision: 0.9918 - val_recall: 0.9917 - val_f1: 0.9918\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02074\n",
      "Epoch 74/100\n",
      " - 1567s - loss: 0.0257 - acc: 0.9908 - precision: 0.9909 - recall: 0.9907 - f1: 0.9908 - val_loss: 0.0247 - val_acc: 0.9923 - val_precision: 0.9925 - val_recall: 0.9923 - val_f1: 0.9924\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02074\n",
      "Epoch 75/100\n",
      " - 1355s - loss: 0.0274 - acc: 0.9903 - precision: 0.9904 - recall: 0.9902 - f1: 0.9903 - val_loss: 0.0219 - val_acc: 0.9931 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02074\n",
      "Epoch 76/100\n",
      " - 1315s - loss: 0.0251 - acc: 0.9906 - precision: 0.9907 - recall: 0.9906 - f1: 0.9906 - val_loss: 0.0236 - val_acc: 0.9927 - val_precision: 0.9927 - val_recall: 0.9927 - val_f1: 0.9927\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02074\n",
      "Epoch 77/100\n",
      " - 1360s - loss: 0.0261 - acc: 0.9905 - precision: 0.9906 - recall: 0.9904 - f1: 0.9905 - val_loss: 0.0234 - val_acc: 0.9924 - val_precision: 0.9924 - val_recall: 0.9923 - val_f1: 0.9923\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02074\n",
      "Epoch 78/100\n",
      " - 1402s - loss: 0.0260 - acc: 0.9908 - precision: 0.9910 - recall: 0.9906 - f1: 0.9908 - val_loss: 0.0239 - val_acc: 0.9930 - val_precision: 0.9930 - val_recall: 0.9929 - val_f1: 0.9930\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02074\n",
      "Epoch 79/100\n",
      " - 1689s - loss: 0.0254 - acc: 0.9910 - precision: 0.9912 - recall: 0.9909 - f1: 0.9910 - val_loss: 0.0295 - val_acc: 0.9905 - val_precision: 0.9906 - val_recall: 0.9905 - val_f1: 0.9905\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02074\n",
      "Epoch 80/100\n",
      " - 1348s - loss: 0.0252 - acc: 0.9912 - precision: 0.9914 - recall: 0.9912 - f1: 0.9913 - val_loss: 0.0295 - val_acc: 0.9910 - val_precision: 0.9911 - val_recall: 0.9910 - val_f1: 0.9910\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02074\n",
      "Epoch 81/100\n",
      " - 1669s - loss: 0.0250 - acc: 0.9910 - precision: 0.9911 - recall: 0.9909 - f1: 0.9910 - val_loss: 0.0265 - val_acc: 0.9923 - val_precision: 0.9923 - val_recall: 0.9922 - val_f1: 0.9923\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02074\n",
      "Epoch 82/100\n",
      " - 1343s - loss: 0.0243 - acc: 0.9920 - precision: 0.9921 - recall: 0.9919 - f1: 0.9920 - val_loss: 0.0251 - val_acc: 0.9920 - val_precision: 0.9921 - val_recall: 0.9919 - val_f1: 0.9920\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02074\n",
      "Epoch 83/100\n",
      " - 1431s - loss: 0.0234 - acc: 0.9914 - precision: 0.9916 - recall: 0.9913 - f1: 0.9914 - val_loss: 0.0214 - val_acc: 0.9929 - val_precision: 0.9930 - val_recall: 0.9929 - val_f1: 0.9929\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02074\n",
      "Epoch 84/100\n",
      " - 1293s - loss: 0.0248 - acc: 0.9908 - precision: 0.9909 - recall: 0.9907 - f1: 0.9908 - val_loss: 0.0242 - val_acc: 0.9924 - val_precision: 0.9924 - val_recall: 0.9923 - val_f1: 0.9924\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02074\n",
      "Epoch 85/100\n",
      " - 1344s - loss: 0.0240 - acc: 0.9915 - precision: 0.9916 - recall: 0.9914 - f1: 0.9915 - val_loss: 0.0209 - val_acc: 0.9926 - val_precision: 0.9928 - val_recall: 0.9926 - val_f1: 0.9927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02074\n",
      "Epoch 86/100\n",
      " - 1315s - loss: 0.0238 - acc: 0.9913 - precision: 0.9914 - recall: 0.9911 - f1: 0.9912 - val_loss: 0.0225 - val_acc: 0.9930 - val_precision: 0.9930 - val_recall: 0.9928 - val_f1: 0.9929\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02074\n",
      "Epoch 87/100\n",
      " - 1319s - loss: 0.0232 - acc: 0.9919 - precision: 0.9921 - recall: 0.9918 - f1: 0.9919 - val_loss: 0.0223 - val_acc: 0.9928 - val_precision: 0.9928 - val_recall: 0.9928 - val_f1: 0.9928\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02074\n",
      "Epoch 88/100\n",
      " - 1336s - loss: 0.0240 - acc: 0.9914 - precision: 0.9915 - recall: 0.9913 - f1: 0.9914 - val_loss: 0.0290 - val_acc: 0.9909 - val_precision: 0.9910 - val_recall: 0.9909 - val_f1: 0.9909\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02074\n",
      "Epoch 89/100\n",
      " - 1342s - loss: 0.0251 - acc: 0.9911 - precision: 0.9913 - recall: 0.9910 - f1: 0.9911 - val_loss: 0.0236 - val_acc: 0.9923 - val_precision: 0.9924 - val_recall: 0.9923 - val_f1: 0.9923\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02074\n",
      "Epoch 90/100\n",
      " - 1513s - loss: 0.0232 - acc: 0.9918 - precision: 0.9919 - recall: 0.9917 - f1: 0.9918 - val_loss: 0.0214 - val_acc: 0.9933 - val_precision: 0.9933 - val_recall: 0.9932 - val_f1: 0.9933\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02074\n",
      "Epoch 91/100\n",
      " - 1239s - loss: 0.0233 - acc: 0.9919 - precision: 0.9919 - recall: 0.9917 - f1: 0.9918 - val_loss: 0.0213 - val_acc: 0.9937 - val_precision: 0.9940 - val_recall: 0.9937 - val_f1: 0.9938\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02074\n",
      "Epoch 92/100\n",
      " - 1318s - loss: 0.0239 - acc: 0.9916 - precision: 0.9918 - recall: 0.9914 - f1: 0.9916 - val_loss: 0.0200 - val_acc: 0.9935 - val_precision: 0.9936 - val_recall: 0.9935 - val_f1: 0.9936\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02074 to 0.01998, saving model to models/heysaw_fold_2.h5\n",
      "Epoch 93/100\n",
      " - 1287s - loss: 0.0237 - acc: 0.9912 - precision: 0.9913 - recall: 0.9911 - f1: 0.9912 - val_loss: 0.0219 - val_acc: 0.9928 - val_precision: 0.9928 - val_recall: 0.9927 - val_f1: 0.9928\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01998\n",
      "Epoch 94/100\n",
      " - 1347s - loss: 0.0228 - acc: 0.9918 - precision: 0.9919 - recall: 0.9916 - f1: 0.9917 - val_loss: 0.0208 - val_acc: 0.9934 - val_precision: 0.9934 - val_recall: 0.9933 - val_f1: 0.9933\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01998\n",
      "Epoch 95/100\n",
      " - 1250s - loss: 0.0227 - acc: 0.9921 - precision: 0.9922 - recall: 0.9919 - f1: 0.9920 - val_loss: 0.0210 - val_acc: 0.9931 - val_precision: 0.9931 - val_recall: 0.9930 - val_f1: 0.9931\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01998\n",
      "Epoch 96/100\n",
      " - 1276s - loss: 0.0224 - acc: 0.9923 - precision: 0.9925 - recall: 0.9923 - f1: 0.9924 - val_loss: 0.0214 - val_acc: 0.9933 - val_precision: 0.9933 - val_recall: 0.9933 - val_f1: 0.9933\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01998\n",
      "Epoch 97/100\n",
      " - 1396s - loss: 0.0247 - acc: 0.9911 - precision: 0.9912 - recall: 0.9910 - f1: 0.9911 - val_loss: 0.0233 - val_acc: 0.9928 - val_precision: 0.9929 - val_recall: 0.9928 - val_f1: 0.9929\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01998\n",
      "Epoch 98/100\n",
      " - 1314s - loss: 0.0225 - acc: 0.9921 - precision: 0.9922 - recall: 0.9920 - f1: 0.9921 - val_loss: 0.0208 - val_acc: 0.9932 - val_precision: 0.9933 - val_recall: 0.9932 - val_f1: 0.9933\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01998\n",
      "Epoch 99/100\n",
      " - 1302s - loss: 0.0238 - acc: 0.9915 - precision: 0.9916 - recall: 0.9914 - f1: 0.9915 - val_loss: 0.0225 - val_acc: 0.9934 - val_precision: 0.9935 - val_recall: 0.9934 - val_f1: 0.9934\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01998\n",
      "Epoch 100/100\n",
      " - 1389s - loss: 0.0236 - acc: 0.9916 - precision: 0.9916 - recall: 0.9915 - f1: 0.9916 - val_loss: 0.0199 - val_acc: 0.9936 - val_precision: 0.9937 - val_recall: 0.9936 - val_f1: 0.9937\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.01998 to 0.01994, saving model to models/heysaw_fold_2.h5\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n",
    "\n",
    "dataset = OCT_TrainingDataGenerator(SHAPE, BATCH_SIZE, 0, DIR, augment=False)\n",
    "\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset)))):\n",
    "    print(\"** FOLD {} **\".format(ix))       \n",
    "    \n",
    "    tg = OCT_TrainingDataGenerator(SHAPE, BATCH_SIZE, train_index, DIR, augment=True)  # Create training data generator with augmentation is true.\n",
    "    vg = OCT_TrainingDataGenerator(SHAPE, BATCH_SIZE, test_index , DIR, augment=False) # Create validation data generator with augmentation is false.\n",
    "        \n",
    "    schedule = SGDRScheduler(min_lr=1e-6,\n",
    "                             max_lr=1e-3,\n",
    "                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n",
    "                             lr_decay=0.8,\n",
    "                             cycle_length=10,\n",
    "                             mult_factor=1.)\n",
    "\n",
    "    model_ckpt = os.path.join(MODEL_SAVE_PATH,\"heysaw_fold_\"+str(ix)+\".h5\")\n",
    "    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n",
    "                 TensorBoard(log_dir=os.path.join(LOGS_PATH,'log_'+str(ix)), update_freq='epoch'), schedule] \n",
    "                                               \n",
    "    model.fit_generator(tg.data_generator(),\n",
    "                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        validation_data=vg.data_generator(),\n",
    "                        validation_steps=len(test_index)//BATCH_SIZE,\n",
    "                        callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    # Load test images' paths.\n",
    "    arr = []\n",
    "    for dir_ in ['test/']:\n",
    "        for y in ['CNV', \"DME\", \"DRUSEN\", \"NORMAL\"]:\n",
    "            for im in os.listdir(os.path.join(DIR,dir_,y)):\n",
    "                arr.append(os.path.join(DIR,dir_,y,im))\n",
    "    \n",
    "    # Get and Store test images and labels.\n",
    "    x = np.empty((len(arr),)+SHAPE, dtype=np.float16)\n",
    "    y = np.empty((len(arr), 4), dtype=np.float16)\n",
    "    \n",
    "    for ix, path in tqdm(enumerate(arr)):\n",
    "        img = np.array(cv2.imread(path))\n",
    "        img = cv2.resize(img, SHAPE[:2])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = normalize(img)\n",
    "        img = np.expand_dims(img, axis=2)\n",
    "\n",
    "        if 'CNV' in path:\n",
    "            label = [1,0,0,0]\n",
    "        elif 'DME' in path:\n",
    "            label = [0,1,0,0]\n",
    "        elif 'DRUSEN' in path:\n",
    "            label = [0,0,1,0]\n",
    "        elif 'NORMAL' in path:\n",
    "            label = [0,0,0,1]\n",
    "            \n",
    "        x[ix] = img\n",
    "        y[ix] = label\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:19, 54.17it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions\n",
    "def threshold_arr(array):\n",
    "    \n",
    "    # Get all predictions from array\n",
    "    # Find \"max confident score location\" in predictions\n",
    "    # Create numpy zeros array\n",
    "    # Just make \"max confident score location\" is 1 in numpy zeros array (other's zero)\n",
    "    # Store thresholded prediction in new array\n",
    "    \n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val)), dtype=np.float16))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** MODEL 0 **\n",
      "[0.05473858994769398, 0.98, 0.98, 0.979, 0.979492036819458]\n",
      "** MODEL 1 **\n",
      "[0.028088031499937643, 0.989, 0.989, 0.989, 0.9889999828338623]\n",
      "** MODEL 2 **\n",
      "[0.037993883370538245, 0.988, 0.988, 0.988, 0.987999984741211]\n"
     ]
    }
   ],
   "source": [
    "# Load all models [3-fold(CV)]\n",
    "models = []\n",
    "scores = []\n",
    "for i in range(3):\n",
    "    print(\"** MODEL {} **\".format(i))\n",
    "    model = load_model(os.path.join(MODEL_SAVE_PATH,\"heysaw_fold_\"+str(i)+\".h5\"), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
    "    score = model.evaluate(x, y, verbose=0)\n",
    "    print(score)\n",
    "    scores.append(score[0])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best scored model according to minimum loss\n",
    "best_model_index = int(np.array(scores).argmin())\n",
    "model = models[best_model_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label\\naccuracy={:0.4f}'.format(accuracy))\n",
    "    plt.savefig(\"stuffs/confusion_matrix.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.989, F1_Score: 0.9890404823606509, Precision: 0.9892547782694662, Recall: 0.989\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       250\n",
      "           1       1.00      0.98      0.99       250\n",
      "           2       0.99      0.98      0.99       250\n",
      "           3       1.00      1.00      1.00       250\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      " samples avg       0.99      0.99      0.99      1000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds = threshold_arr(model.predict(x, verbose=0))\n",
    "\n",
    "results = precision_recall_fscore_support(y, y_preds ,average='macro')\n",
    "acc = accuracy_score(y, y_preds)\n",
    "\n",
    "print(\"Accuracy: {}, F1_Score: {}, Precision: {}, Recall: {}\".format(acc, results[2], results[0], results[1]))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y, y_preds))\n",
    "print(\"\\n\")\n",
    "cnf_matrix = confusion_matrix(y.argmax(axis=1), y_preds.argmax(axis=1))\n",
    "\n",
    "plot_confusion_matrix(cm           = cnf_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['DME', 'CNV', 'DRUSEN', 'NORMAL'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]]\n",
      "373.9151954650879 ms\n",
      "***\n",
      "[[0. 0. 0. 1.]]\n",
      "12.000799179077148 ms\n",
      "***\n",
      "[[0. 0. 0. 1.]]\n",
      "11.966466903686523 ms\n",
      "***\n",
      "[[0. 0. 0. 1.]]\n",
      "12.933969497680664 ms\n",
      "***\n",
      "[[0. 0. 0. 1.]]\n",
      "11.968612670898438 ms\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    img = np.array(cv2.imread(DIR+\"test/NORMAL/NORMAL-2434258-1.jpeg\"))\n",
    "    img = cv2.resize(img, SHAPE[:2])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = normalize(img)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    \n",
    "    start = time.time()\n",
    "    prediction = model.predict(np.expand_dims(img, axis=0), batch_size=1)\n",
    "    finish = time.time()\n",
    "    \n",
    "    print(threshold_arr(prediction))\n",
    "    print((finish-start)*1000,\"ms\")\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
